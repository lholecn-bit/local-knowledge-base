# æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿ - éƒ¨ç½²å’Œä½¿ç”¨æ–‡æ¡£

## ç›®å½•

1. [ç³»ç»Ÿè¦æ±‚](#ç³»ç»Ÿè¦æ±‚)
2. [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
3. [é…ç½®è¯´æ˜](#é…ç½®è¯´æ˜)
4. [éƒ¨ç½²æ–¹å¼](#éƒ¨ç½²æ–¹å¼)
5. [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
6. [API æ–‡æ¡£](#api-æ–‡æ¡£)
7. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)
8. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## ç³»ç»Ÿè¦æ±‚

### ç¡¬ä»¶è¦æ±‚

| é¡¹ç›® | æœ€ä½é…ç½®       | æ¨èé…ç½® |
| ---- | -------------- | -------- |
| CPU  | 2æ ¸            | 4æ ¸+     |
| å†…å­˜ | 4GB            | 8GB+     |
| å­˜å‚¨ | 10GB           | 50GB+    |
| ç½‘ç»œ | ç¨³å®šäº’è”ç½‘è¿æ¥ | 20Mbps+  |

### è½¯ä»¶è¦æ±‚

```
- Python 3.8+
- pip æˆ– conda
- Git
```

### äº‘ç«¯æœåŠ¡ï¼ˆé€‰æ‹©ä¸€ä¸ªæˆ–å¤šä¸ªï¼‰

#### Embedding æœåŠ¡

- **OpenAI**: text-embedding-3-small / text-embedding-3-large
- **é˜¿é‡Œäº‘é€šä¹‰**: text-embedding-v1 / text-embedding-v2
- **æ™ºè°± AI**: embedding-2
- **Ollama**: è‡ªéƒ¨ç½²ï¼ˆæ”¯æŒç¦»çº¿ï¼‰

#### LLM æœåŠ¡

- **OpenAI**: gpt-3.5-turbo / gpt-4
- **Anthropic Claude**: claude-3-opus / claude-3-sonnet
- **é˜¿é‡Œäº‘é€šä¹‰**: qwen-plus / qwen-max
- **æ™ºè°± AI**: glm-3-turbo / glm-4

---

## å®‰è£…æ­¥éª¤

### 1. å…‹éš†é¡¹ç›®

```bash
git clone https://github.com/your-repo/local-knowledge-base.git
cd local-knowledge-base
```

### 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

#### ä½¿ç”¨ venvï¼ˆæ¨èï¼‰

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Windows
venv\Scripts\activate
# macOS/Linux
source venv/bin/activate
```

#### æˆ–ä½¿ç”¨ conda

```bash
conda create -n kb-env python=3.10
conda activate kb-env
```

### 3. å®‰è£…ä¾èµ–

```bash
# åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# æˆ–è€…æ‰‹åŠ¨å®‰è£…
pip install langchain langchain-core langchain-community
pip install openai anthropic
pip install flask flask-cors
pip install python-dotenv
pip install pypdf python-docx markdown2
pip install chromadb
pip install requests
pip install zhipuai dashscope
```

### 4. é¡¹ç›®ç»“æ„

```
local-knowledge-base/
â”œâ”€â”€ config.py              # é…ç½®æ–‡ä»¶
â”œâ”€â”€ embeddings.py          # Embedding é€‚é…å±‚
â”œâ”€â”€ llm.py                 # LLM é€‚é…å±‚
â”œâ”€â”€ knowledge_base.py      # æ ¸å¿ƒçŸ¥è¯†åº“
â”œâ”€â”€ app.py                 # Flask API
â”œâ”€â”€ requirements.txt       # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ .env.example           # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ .env                   # ç¯å¢ƒå˜é‡é…ç½®ï¼ˆæœ¬åœ°ï¼‰
â”œâ”€â”€ knowledge_db/          # å‘é‡æ•°æ®åº“ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
â”œâ”€â”€ docs/                  # æ–‡æ¡£ç›®å½•
â”œâ”€â”€ examples/              # ç¤ºä¾‹è„šæœ¬
â””â”€â”€ README.md
```

---

## é…ç½®è¯´æ˜

### 1. ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼ˆå¤åˆ¶ `.env.example`ï¼‰ï¼š

```bash
cp .env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼š

```env
# ============ OpenAI é…ç½® ============
OPENAI_API_KEY=sk-xxxxxxxxxxxxx
OPENAI_API_BASE=https://api.openai.com/v1

# ============ é˜¿é‡Œäº‘é€šä¹‰ é…ç½® ============
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxx

# ============ æ™ºè°± AI é…ç½® ============
ZHIPU_API_KEY=xxxxxxxxxxxxx

# ============ Anthropic Claude é…ç½® ============
CLAUDE_API_KEY=sk-ant-xxxxxxxxxxxxx

# ============ Ollama é…ç½®ï¼ˆæœ¬åœ°éƒ¨ç½²ï¼‰ ============
OLLAMA_API_BASE=http://localhost:11434

# ============ LLM é…ç½® ============
LLM_PROVIDER=openai              # openai / claude / zhipu / qwen
LLM_MODEL=gpt-3.5-turbo
LLM_API_KEY=${OPENAI_API_KEY}
LLM_API_BASE=${OPENAI_API_BASE}

# ============ Embedding é…ç½® ============
EMBEDDING_PROVIDER=openai        # openai / zhipu / qwen / ollama
EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_API_KEY=${OPENAI_API_KEY}
EMBEDDING_API_BASE=${OPENAI_API_BASE}

# ============ æ•°æ®åº“é…ç½® ============
VECTOR_DB_PATH=./knowledge_db
VECTOR_DB_TYPE=chroma            # chroma / milvus / weaviate

# ============ åº”ç”¨é…ç½® ============
FLASK_ENV=development            # development / production
FLASK_DEBUG=False
LOG_LEVEL=INFO
```

### 2. è¯¦ç»†é…ç½®ç¤ºä¾‹

#### 2.1 ä½¿ç”¨ OpenAI

```env
OPENAI_API_KEY=sk-proj-xxxxxxxxxxxxx
LLM_PROVIDER=openai
LLM_MODEL=gpt-3.5-turbo
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
```

#### 2.2 ä½¿ç”¨é˜¿é‡Œäº‘é€šä¹‰

```env
DASHSCOPE_API_KEY=sk-xxxxxxxxxxxxx
LLM_PROVIDER=qwen
LLM_MODEL=qwen-max
EMBEDDING_PROVIDER=qwen
EMBEDDING_MODEL=text-embedding-v2
```

#### 2.3 ä½¿ç”¨æ™ºè°± AI

```env
ZHIPU_API_KEY=xxxxxxxxxxxxx
LLM_PROVIDER=zhipu
LLM_MODEL=glm-4
EMBEDDING_PROVIDER=zhipu
EMBEDDING_MODEL=embedding-2
```

#### 2.4 ä½¿ç”¨ Claude

```env
CLAUDE_API_KEY=sk-ant-xxxxxxxxxxxxx
LLM_PROVIDER=claude
LLM_MODEL=claude-3-sonnet-20240229
EMBEDDING_PROVIDER=openai
EMBEDDING_MODEL=text-embedding-3-small
```

#### 2.5 æ··åˆé…ç½®ï¼ˆæœ¬åœ° Embedding + äº‘ç«¯ LLMï¼‰

```env
# ä½¿ç”¨æœ¬åœ° Ollama è¿›è¡Œå‘é‡åŒ–
OLLAMA_API_BASE=http://localhost:11434
EMBEDDING_PROVIDER=ollama
EMBEDDING_MODEL=nomic-embed-text

# ä½¿ç”¨ OpenAI è¿›è¡Œå›ç­”
OPENAI_API_KEY=sk-xxxxxxxxxxxxx
LLM_PROVIDER=openai
LLM_MODEL=gpt-3.5-turbo
```

### 3. config.py é…ç½®

ç¼–è¾‘ `config.py` ä¸­çš„é…ç½®ç±»ï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æƒ³ä½¿ç”¨ .envï¼‰ï¼š

```python
# config.py
import os
from dotenv import load_dotenv

load_dotenv()

# æ ¹æ®ç¯å¢ƒå˜é‡è®¾ç½®
@dataclass
class EmbeddingConfig:
    provider: str = os.getenv("EMBEDDING_PROVIDER", "openai")
    model: str = os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
    api_key: str = os.getenv("OPENAI_API_KEY", "")
    api_base: str = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
    batch_size: int = 100

@dataclass
class LLMConfig:
    provider: str = os.getenv("LLM_PROVIDER", "openai")
    model: str = os.getenv("LLM_MODEL", "gpt-3.5-turbo")
    api_key: str = os.getenv("LLM_API_KEY", "")
    api_base: str = os.getenv("LLM_API_BASE", "")
    temperature: float = 0.7
    max_tokens: int = 2048
```

---

## éƒ¨ç½²æ–¹å¼

### æ–¹å¼ 1: æœ¬åœ°å¼€å‘éƒ¨ç½²

#### 1.1 å•æœºéƒ¨ç½²ï¼ˆæ¨èç”¨äºå¼€å‘æµ‹è¯•ï¼‰

```bash
# 1. æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export FLASK_APP=app.py
export FLASK_ENV=development

# 3. è¿è¡Œåº”ç”¨
python app.py
```

è®¿é—®åœ°å€ï¼š`http://localhost:5000`

#### 1.2 ä½¿ç”¨ Gunicorn éƒ¨ç½²ï¼ˆæ¨èç”¨äºç”Ÿäº§ï¼‰

```bash
# å®‰è£… Gunicorn
pip install gunicorn

# è¿è¡Œåº”ç”¨
gunicorn -w 4 -b 0.0.0.0:5000 app:app
```

å‚æ•°è¯´æ˜ï¼š

- `-w 4`: 4ä¸ªå·¥ä½œè¿›ç¨‹
- `-b 0.0.0.0:5000`: ç»‘å®šåœ°å€å’Œç«¯å£
- `--timeout 300`: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
- `--access-logfile -`: è¾“å‡ºè®¿é—®æ—¥å¿—
- `--error-logfile -`: è¾“å‡ºé”™è¯¯æ—¥å¿—

```bash
gunicorn -w 4 -b 0.0.0.0:5000 --timeout 300 --access-logfile - app:app
```

---

### æ–¹å¼ 2: Docker éƒ¨ç½²

#### 2.1 åˆ›å»º Dockerfile

```dockerfile
# Dockerfile
FROM python:3.10-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    build-essential \
    curl \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºæ•°æ®å·æŒ‚è½½ç‚¹
VOLUME ["/app/knowledge_db"]

# æš´éœ²ç«¯å£
EXPOSE 5000

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD curl -f http://localhost:5000/api/health || exit 1

# å¯åŠ¨åº”ç”¨
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "--timeout", "300", "app:app"]
```

#### 2.2 åˆ›å»º docker-compose.yml

```yaml
# docker-compose.yml
version: '3.8'

services:
  knowledge-base:
    build: .
    container_name: local-kb
    ports:
      - "5000:5000"
    volumes:
      - ./knowledge_db:/app/knowledge_db
      - ./docs:/app/docs
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_PROVIDER=openai
      - LLM_MODEL=gpt-3.5-turbo
      - EMBEDDING_PROVIDER=openai
      - EMBEDDING_MODEL=text-embedding-3-small
      - FLASK_ENV=production
    restart: always
  
  # å¯é€‰ï¼šOllama æœ¬åœ°éƒ¨ç½²
  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
    restart: always

volumes:
  ollama_data:
```

#### 2.3 ä½¿ç”¨ Docker Compose éƒ¨ç½²

```bash
# 1. åˆ›å»º .env æ–‡ä»¶
cp .env.example .env
# ç¼–è¾‘ .envï¼Œå¡«å…¥ API Key

# 2. å¯åŠ¨æœåŠ¡
docker-compose up -d

# 3. æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f knowledge-base

# 4. åœæ­¢æœåŠ¡
docker-compose down
```

---

### æ–¹å¼ 3: Kubernetes éƒ¨ç½²

#### 3.1 åˆ›å»º Kubernetes é…ç½®

```yaml
# k8s-deployment.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: kb-config
data:
  FLASK_ENV: "production"
  LLM_PROVIDER: "openai"
  EMBEDDING_PROVIDER: "openai"

---
apiVersion: v1
kind: Secret
metadata:
  name: kb-secrets
type: Opaque
stringData:
  OPENAI_API_KEY: "sk-xxxxxxxxxxxxx"

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: knowledge-base
spec:
  replicas: 3
  selector:
    matchLabels:
      app: knowledge-base
  template:
    metadata:
      labels:
        app: knowledge-base
    spec:
      containers:
      - name: knowledge-base
        image: your-registry/knowledge-base:latest
        ports:
        - containerPort: 5000
        envFrom:
        - configMapRef:
            name: kb-config
        - secretRef:
            name: kb-secrets
        resources:
          requests:
            cpu: "500m"
            memory: "1Gi"
          limits:
            cpu: "1000m"
            memory: "2Gi"
        livenessProbe:
          httpGet:
            path: /api/health
            port: 5000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /api/health
            port: 5000
          initialDelaySeconds: 10
          periodSeconds: 5
        volumeMounts:
        - name: kb-data
          mountPath: /app/knowledge_db
      volumes:
      - name: kb-data
        persistentVolumeClaim:
          claimName: kb-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: knowledge-base-service
spec:
  type: LoadBalancer
  ports:
  - port: 80
    targetPort: 5000
  selector:
    app: knowledge-base

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: kb-pvc
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi
```

#### 3.2 éƒ¨ç½²åˆ° Kubernetes

```bash
# 1. æ„å»ºé•œåƒå¹¶æ¨é€åˆ°ä»“åº“
docker build -t your-registry/knowledge-base:latest .
docker push your-registry/knowledge-base:latest

# 2. éƒ¨ç½²åˆ° K8s
kubectl apply -f k8s-deployment.yaml

# 3. æŸ¥çœ‹éƒ¨ç½²çŠ¶æ€
kubectl get pods
kubectl get svc

# 4. æŸ¥çœ‹æ—¥å¿—
kubectl logs -f deployment/knowledge-base

# 5. è®¿é—®æœåŠ¡
kubectl port-forward svc/knowledge-base-service 5000:80
```

---

### æ–¹å¼ 4: äº‘å¹³å°éƒ¨ç½²

#### 4.1 éƒ¨ç½²åˆ°é˜¿é‡Œäº‘ ECS

```bash
# 1. è¿æ¥åˆ° ECS å®ä¾‹
ssh -i your-key.pem root@your-instance-ip

# 2. å®‰è£…ä¾èµ–
apt-get update
apt-get install -y python3.10 python3-pip python3-venv git

# 3. å…‹éš†é¡¹ç›®
git clone https://github.com/your-repo/local-knowledge-base.git
cd local-knowledge-base

# 4. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# 5. å®‰è£…ä¾èµ–
pip install -r requirements.txt

# 6. é…ç½®ç¯å¢ƒå˜é‡
nano .env
# å¡«å…¥ API Key å’Œé…ç½®

# 7. ä½¿ç”¨ systemd ç®¡ç†æœåŠ¡
sudo nano /etc/systemd/system/knowledge-base.service
```

ç¼–è¾‘ `/etc/systemd/system/knowledge-base.service`ï¼š

```ini
[Unit]
Description=Local Knowledge Base Service
After=network.target

[Service]
Type=notify
User=root
WorkingDirectory=/root/local-knowledge-base
Environment="PATH=/root/local-knowledge-base/venv/bin"
EnvironmentFile=/root/local-knowledge-base/.env
ExecStart=/root/local-knowledge-base/venv/bin/gunicorn \
    -w 4 \
    -b 0.0.0.0:5000 \
    --timeout 300 \
    --access-logfile /var/log/kb/access.log \
    --error-logfile /var/log/kb/error.log \
    app:app
Restart=always
RestartSec=10

[Install]
WantedBy=multi-user.target
```

å¯åŠ¨æœåŠ¡ï¼š

```bash
# åˆ›å»ºæ—¥å¿—ç›®å½•
mkdir -p /var/log/kb
chmod 755 /var/log/kb

# å¯åŠ¨æœåŠ¡
sudo systemctl start knowledge-base
sudo systemctl enable knowledge-base

# æŸ¥çœ‹çŠ¶æ€
sudo systemctl status knowledge-base

# æŸ¥çœ‹æ—¥å¿—
tail -f /var/log/kb/error.log
```

#### 4.2 éƒ¨ç½²åˆ° AWS Lambdaï¼ˆæ— æœåŠ¡å™¨ï¼‰

```python
# lambda_handler.py
from knowledge_base import LocalKnowledgeBase
from config import EmbeddingConfig, LLMConfig
import json
import os

# å…¨å±€åˆå§‹åŒ–ï¼ˆLambda ä¼šå¤ç”¨ï¼‰
kb = None

def init_kb():
    global kb
    if kb is None:
        embedding_config = EmbeddingConfig(
            provider=os.getenv("EMBEDDING_PROVIDER", "openai"),
            model=os.getenv("EMBEDDING_MODEL"),
            api_key=os.getenv("OPENAI_API_KEY"),
        )
        llm_config = LLMConfig(
            provider=os.getenv("LLM_PROVIDER", "openai"),
            model=os.getenv("LLM_MODEL"),
            api_key=os.getenv("OPENAI_API_KEY"),
        )
        kb = LocalKnowledgeBase(
            embedding_config=embedding_config,
            llm_config=llm_config,
        )

def lambda_handler(event, context):
    """Lambda å¤„ç†å‡½æ•°"""
    try:
        init_kb()
      
        body = json.loads(event.get("body", "{}"))
        action = body.get("action")
      
        if action == "query":
            result = kb.query(
                question=body.get("question"),
                top_k=body.get("top_k", 3),
            )
            return {
                "statusCode": 200,
                "body": json.dumps(result, ensure_ascii=False),
            }
      
        elif action == "search":
            results = kb.similarity_search(
                query=body.get("query"),
                top_k=body.get("top_k", 5),
            )
            return {
                "statusCode": 200,
                "body": json.dumps({"results": results}, ensure_ascii=False),
            }
      
        else:
            return {
                "statusCode": 400,
                "body": json.dumps({"error": "æœªçŸ¥çš„ action"}),
            }
  
    except Exception as e:
        return {
            "statusCode": 500,
            "body": json.dumps({"error": str(e)}),
        }
```

---

## ä½¿ç”¨æŒ‡å—

### 1. å¿«é€Ÿå¼€å§‹è„šæœ¬

åˆ›å»º `quick_start.py`ï¼š

```python
# quick_start.py
"""å¿«é€Ÿå¼€å§‹ç¤ºä¾‹"""

from knowledge_base import LocalKnowledgeBase
from config import EmbeddingConfig, LLMConfig, VECTOR_STORE_CONFIG

def main():
    print("=" * 60)
    print("æœ¬åœ°çŸ¥è¯†åº“ç³»ç»Ÿ - å¿«é€Ÿå¼€å§‹")
    print("=" * 60)
  
    # 1. åˆå§‹åŒ–çŸ¥è¯†åº“
    print("\n[1/4] åˆå§‹åŒ–çŸ¥è¯†åº“...")
  
    embedding_config = EmbeddingConfig(
        provider="openai",
        model="text-embedding-3-small",
    )
  
    llm_config = LLMConfig(
        provider="openai",
        model="gpt-3.5-turbo",
    )
  
    kb = LocalKnowledgeBase(
        embedding_config=embedding_config,
        llm_config=llm_config,
        vector_store_config=VECTOR_STORE_CONFIG,
    )
  
    # 2. æ·»åŠ æ–‡æ¡£
    print("\n[2/4] æ·»åŠ æ–‡æ¡£...")
    print("è¯·æ”¾ç½®æ–‡æ¡£åˆ° ./docs ç›®å½•ï¼Œæ”¯æŒ PDFã€TXTã€Markdown æ ¼å¼")
  
    doc_paths = ["./docs"]  # ä¿®æ”¹ä¸ºå®é™…è·¯å¾„
    count = kb.add_documents(doc_paths)
    print(f"âœ“ æˆåŠŸæ·»åŠ  {count} ä¸ªæ–‡æœ¬å—")
  
    # 3. æ‰§è¡ŒæŸ¥è¯¢
    print("\n[3/4] æ‰§è¡ŒæŸ¥è¯¢...")
    question = input("è¯·è¾“å…¥é—®é¢˜ï¼ˆæˆ–æŒ‰ Enter ä½¿ç”¨é»˜è®¤é—®é¢˜ï¼‰: ").strip()
    if not question:
        question = "è¯·æ€»ç»“ä¸€ä¸‹æ–‡æ¡£çš„ä¸»è¦å†…å®¹"
  
    result = kb.query(question, top_k=3)
  
    print(f"\nğŸ“Œ é—®é¢˜: {result['question']}")
    print(f"\nğŸ’¬ å›ç­”:\n{result['answer']}")
  
    print(f"\nğŸ“š ç›¸å…³æ–‡æ¡£ ({len(result['source_documents'])} ä¸ª):")
    for i, doc in enumerate(result['source_documents'], 1):
        print(f"\n  [{i}] {doc['content'][:150]}...")
        print(f"      æ¥æº: {doc['metadata']}")
  
    # 4. ç›¸ä¼¼åº¦æœç´¢
    print("\n[4/4] ç›¸ä¼¼åº¦æœç´¢...")
    query = input("è¯·è¾“å…¥æœç´¢å…³é”®è¯: ").strip()
    if query:
        similar_docs = kb.similarity_search(query, top_k=3)
        print(f"\næœç´¢ç»“æœ ({len(similar_docs)} ä¸ª):")
        for i, doc in enumerate(similar_docs, 1):
            print(f"\n  [{i}] ç›¸ä¼¼åº¦: {doc['score']:.3f}")
            print(f"      å†…å®¹: {doc['content'][:150]}...")


if __name__ == "__main__":
    main()
```

è¿è¡Œï¼š

```bash
python quick_start.py
```

---

### 2. Python é›†æˆç¤ºä¾‹

#### 2.1 åŸºç¡€ä½¿ç”¨

```python
# example_basic.py
from knowledge_base import LocalKnowledgeBase

# åˆå§‹åŒ–
kb = LocalKnowledgeBase()

# æ·»åŠ æ–‡æ¡£
kb.add_documents([
    "./docs/document1.pdf",
    "./docs/document2.txt",
    "./docs/folder",
])

# æŸ¥è¯¢
result = kb.query("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ")
print(f"é—®é¢˜: {result['question']}")
print(f"å›ç­”: {result['answer']}")

# æœç´¢
docs = kb.similarity_search("ç¥ç»ç½‘ç»œ", top_k=5)
for doc in docs:
    print(f"ç›¸ä¼¼åº¦: {doc['score']:.3f}, å†…å®¹: {doc['content'][:100]}")
```

#### 2.2 æµå¼å“åº”

```python
# example_stream.py
from knowledge_base import LocalKnowledgeBase

kb = LocalKnowledgeBase()

# æµå¼æŸ¥è¯¢
print("å›ç­”: ", end="", flush=True)
for chunk in kb.stream_query("è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„åŸç†"):
    print(chunk, end="", flush=True)
print()
```

#### 2.3 æ‰¹é‡å¤„ç†

```python
# example_batch.py
from knowledge_base import LocalKnowledgeBase

kb = LocalKnowledgeBase()

# æ·»åŠ å¤§é‡æ–‡æ¡£
import glob
pdf_files = glob.glob("./docs/**/*.pdf", recursive=True)
kb.add_documents(pdf_files)

# æ‰¹é‡æŸ¥è¯¢
questions = [
    "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ",
    "æœºå™¨å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
    "æ·±åº¦å­¦ä¹ å’Œç¥ç»ç½‘ç»œæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
]

for question in questions:
    result = kb.query(question, top_k=3)
    print(f"\nQ: {question}")
    print(f"A: {result['answer'][:200]}...")
```

---

### 3. API ä½¿ç”¨ç¤ºä¾‹

#### 3.1 ä½¿ç”¨ curl

```bash
# å¯åŠ¨æœåŠ¡
python app.py

# å¥åº·æ£€æŸ¥
curl http://localhost:5000/api/health

# æŸ¥è¯¢
curl -X POST http://localhost:5000/api/query \
  -H "Content-Type: application/json" \
  -d '{
    "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
    "top_k": 3
  }'

# æ·»åŠ æ–‡æ¡£
curl -X POST http://localhost:5000/api/add-documents \
  -H "Content-Type: application/json" \
  -d '{
    "doc_paths": ["./docs/example.pdf"]
  }'

# ç›¸ä¼¼åº¦æœç´¢
curl -X POST http://localhost:5000/api/search \
  -H "Content-Type: application/json" \
  -d '{
    "query": "ç¥ç»ç½‘ç»œ",
    "top_k": 5
  }'

# æ¸…ç©ºçŸ¥è¯†åº“
curl -X POST http://localhost:5000/api/clear-db
```

#### 3.2 ä½¿ç”¨ Python requests

```python
# example_api.py
import requests
import json

BASE_URL = "http://localhost:5000"

def query(question, top_k=3):
    """æŸ¥è¯¢çŸ¥è¯†åº“"""
    response = requests.post(
        f"{BASE_URL}/api/query",
        json={"question": question, "top_k": top_k}
    )
    return response.json()

def add_documents(doc_paths):
    """æ·»åŠ æ–‡æ¡£"""
    response = requests.post(
        f"{BASE_URL}/api/add-documents",
        json={"doc_paths": doc_paths}
    )
    return response.json()

def search(query_text, top_k=5):
    """æœç´¢"""
    response = requests.post(
        f"{BASE_URL}/api/search",
        json={"query": query_text, "top_k": top_k}
    )
    return response.json()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # æ·»åŠ æ–‡æ¡£
    print("æ·»åŠ æ–‡æ¡£...")
    result = add_documents(["./docs"])
    print(f"âœ“ æ·»åŠ äº† {result['added_chunks']} ä¸ªæ–‡æœ¬å—")
  
    # æŸ¥è¯¢
    print("\næŸ¥è¯¢...")
    result = query("ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ", top_k=3)
    print(f"é—®é¢˜: {result['question']}")
    print(f"å›ç­”: {result['answer']}")
  
    # æœç´¢
    print("\næœç´¢...")
    result = search("ç¥ç»ç½‘ç»œ", top_k=3)
    print(f"æ‰¾åˆ° {len(result['results'])} ä¸ªç»“æœ")
    for doc in result['results']:
        print(f"  - {doc['content'][:100]}...")
```

#### 3.3 ä½¿ç”¨ JavaScript/Node.js

```javascript
// example_api.js
const fetch = require('node-fetch');

const BASE_URL = 'http://localhost:5000';

async function query(question, topK = 3) {
    const response = await fetch(`${BASE_URL}/api/query`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ question, top_k: topK })
    });
    return response.json();
}

async function search(queryText, topK = 5) {
    const response = await fetch(`${BASE_URL}/api/search`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ query: queryText, top_k: topK })
    });
    return response.json();
}

// ä½¿ç”¨ç¤ºä¾‹
(async () => {
    const result = await query('ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ', 3);
    console.log('é—®é¢˜:', result.question);
    console.log('å›ç­”:', result.answer);
})();
```

#### 3.4 æµå¼ API è°ƒç”¨

```python
# example_stream_api.py
import requests
import json

BASE_URL = "http://localhost:5000"

def stream_query(question, top_k=3):
    """æµå¼æŸ¥è¯¢"""
    response = requests.post(
        f"{BASE_URL}/api/stream-query",
        json={"question": question, "top_k": top_k},
        stream=True
    )
  
    for line in response.iter_lines():
        if line:
            data = json.loads(line)
            if "chunk" in data:
                yield data["chunk"]

# ä½¿ç”¨ç¤ºä¾‹
print("å›ç­”: ", end="", flush=True)
for chunk in stream_query("è§£é‡Šä¸€ä¸‹æ·±åº¦å­¦ä¹ "):
    print(chunk, end="", flush=True)
print()
```

---

## API æ–‡æ¡£

### 1. æŸ¥è¯¢æ¥å£

**è¯·æ±‚**

```
POST /api/query
Content-Type: application/json

{
  "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
  "top_k": 3
}
```

**å“åº”**

```json
{
  "question": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
  "answer": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé‡è¦åˆ†æ”¯...",
  "source_documents": [
    {
      "content": "æœºå™¨å­¦ä¹ æ˜¯ä½¿è®¡ç®—æœºç³»ç»Ÿèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å’Œæ”¹è¿›ï¼Œè€Œæ— éœ€è¢«æ˜ç¡®ç¼–ç¨‹çš„ç§‘å­¦...",
      "metadata": {
        "source": "/path/to/document.pdf",
        "page": 1
      }
    }
  ]
}
```

**å‚æ•°è¯´æ˜**

| å‚æ•°     | ç±»å‹    | å¿…éœ€ | è¯´æ˜                       |
| -------- | ------- | ---- | -------------------------- |
| question | string  | âœ“   | é—®é¢˜                       |
| top_k    | integer |      | è¿”å›çš„ç›¸å…³æ–‡æ¡£æ•°ï¼ˆé»˜è®¤ 3ï¼‰ |

**è¿”å›å€¼**

| å­—æ®µ     | ç±»å‹   | è¯´æ˜     |
| -------- | ------ | -------- |
| question | string | åŸå§‹é—®é¢˜ |
| answer   |        |          |
