# 机器学习和神经网络基础

## 第一部分：机器学习基础

### 机器学习的定义

机器学习是人工智能的一个分支，使计算机系统能够从数据中学习和改进，而不需要被明确编程。

### 机器学习的三大类型

#### 1. 监督学习（Supervised Learning）

在监督学习中，训练数据包含输入特征和对应的输出标签。

**常见算法**：
- **线性回归** - 预测连续值
- **逻辑回归** - 二分类问题
- **支持向量机（SVM）** - 分类问题
- **决策树** - 分类和回归
- **随机森林** - 集成学习
- **神经网络** - 复杂模式识别

**应用案例**：
- 邮件分类（垃圾/非垃圾）
- 房价预测
- 图像识别
- 情感分析

#### 2. 无监督学习（Unsupervised Learning）

无监督学习处理没有标签的数据，目标是发现数据中的隐藏模式。

**常见算法**：
- **K-Means 聚类** - 将数据分组
- **层次聚类** - 建立聚类树
- **主成分分析（PCA）** - 降维
- **异常检测** - 发现异常数据

**应用案例**：
- 客户分类
- 数据压缩
- 推荐系统
- 欺诈检测

#### 3. 强化学习（Reinforcement Learning）

智能体通过与环境交互，从奖励信号中学习最优策略。

**核心概念**：
- **状态** - 环境当前的情况
- **行为** - 智能体可采取的动作
- **奖励** - 行为的即时反馈
- **策略** - 状态到行为的映射

**应用案例**：
- 游戏 AI（AlphaGo）
- 机器人控制
- 自动驾驶

---

## 第二部分：神经网络基础

### 什么是神经网络

神经网络是受生物神经系统启发的计算模型。它由相互连接的节点（神经元）组成，可以学习复杂的非线性函数。

### 神经网络的基本结构

```
输入层        隐藏层         输出层
(n个节点)    (m个节点)      (k个节点)
  ●───────→  ●─┐
  │            │ ├───→  ●
  ●───────→  ●─┤       
  │            │ └───→  ●
  ●───────→  ●
```

**三层结构**：
1. **输入层** - 接收原始数据
2. **隐藏层** - 进行特征转换和学习
3. **输出层** - 产生预测结果

### 前向传播（Forward Propagation）

前向传播是将输入数据通过网络进行计算的过程。

**计算过程**：
```
z = w * x + b        (线性变换)
a = activation(z)    (非线性激活)
```

**激活函数的作用**：
1. 引入非线性
2. 压缩输出范围
3. 加速收敛

**常见激活函数**：

| 函数 | 公式 | 范围 | 用途 |
|------|------|------|------|
| ReLU | max(0, x) | [0, ∞) | 隐藏层（推荐）|
| Sigmoid | 1/(1+e^(-x)) | (0, 1) | 二分类输出 |
| Tanh | (e^x-e^(-x))/(e^x+e^(-x)) | (-1, 1) | 隐藏层 |
| Softmax | e^xi/Σe^xj | (0, 1) | 多分类输出 |

### 反向传播（Backpropagation）

反向传播是训练神经网络的关键算法。它通过计算损失函数对每个参数的梯度来更新权重。

**步骤**：
1. 前向传播计算预测值
2. 计算损失函数
3. 反向计算梯度
4. 更新权重参数

**梯度下降**：
```
w_new = w_old - learning_rate * dL/dw
```

### 损失函数

损失函数衡量模型预测与实际标签之间的差异。

**常见损失函数**：

```
1. 均方误差（MSE）- 回归
   L = 1/n * Σ(y_pred - y_true)^2

2. 交叉熵（Cross-Entropy）- 分类
   L = -Σ(y_true * log(y_pred))

3. 二进制交叉熵 - 二分类
   L = -[y*log(p) + (1-y)*log(1-p)]
```

---

## 第三部分：深度神经网络

### 为什么需要深层网络

浅层网络表达能力有限，需要指数级多的节点才能表示复杂函数。深层网络通过多层非线性变换，可以高效地表示复杂函数。

**通用逼近定理**：
- 单隐藏层网络可以逼近任何连续函数
- 但可能需要非常多的节点
- 深层网络用少得多的节点可以实现相同的表达能力

### 常见深度学习模型

#### 1. 卷积神经网络（CNN）

**适用**：图像数据

**核心概念**：
- **卷积** - 提取局部特征
- **池化** - 降低维度，增加不变性
- **全连接层** - 进行分类

**工作流程**：
```
输入图像 → 卷积层 → 激活函数 → 池化层 → ... → 全连接层 → 输出
```

**经典架构**：
- LeNet - 手写数字识别
- AlexNet - ImageNet 突破
- VGGNet - 简洁优雅
- ResNet - 残差连接
- MobileNet - 轻量级

#### 2. 循环神经网络（RNN）

**适用**：序列数据（文本、时间序列）

**核心特性**：
- 具有记忆能力
- 处理可变长度输入
- 时间步之间共享权重

**变体**：
- **LSTM** - 长短期记忆，解决梯度消失问题
- **GRU** - 门控循环单元，LSTM 的简化版

**应用**：
- 机器翻译
- 文本生成
- 时间序列预测

#### 3. Transformer

**革新性架构**：
- 基于自注意力机制
- 抛弃递归，实现全并行化
- 更高效的训练

**核心组件**：
- **自注意力** - 学习词与词之间的关系
- **位置编码** - 保留序列位置信息
- **多头注意力** - 多个注意力投射

**应用**：
- BERT - 语言理解
- GPT - 文本生成
- T5 - 序列到序列

---

## 第四部分：优化和正则化

### 过拟合问题

模型在训练集上表现好，但在测试集上表现差。

**表现**：
- 训练损失继续下降
- 验证损失开始上升
- 训练和验证准确度差距大

**原因**：
- 模型过于复杂
- 训练数据不足
- 没有正则化

### 正则化技术

#### 1. L1/L2 正则化

```
L_total = L_original + λ * L_regularization

L1: λ * Σ|w|           (稀疏性)
L2: λ * Σw^2           (平滑性)
```

#### 2. Dropout

在训练时随机删除神经元，强制网络学习冗余表示。

```
训练时：以概率 p 随机将一些激活值设为 0
测试时：使用所有激活值，但乘以 (1-p) 缩放
```

#### 3. 批量归一化（Batch Normalization）

将每个小批数据的输入标准化，加速收敛。

```
x_norm = (x - mean) / sqrt(variance + ε)
y = γ * x_norm + β
```

#### 4. 早停（Early Stopping）

监控验证损失，当不再改善时停止训练。

### 优化算法

| 算法 | 特点 | 学习率 |
|------|------|--------|
| SGD | 基础方法 | 0.01-0.1 |
| Momentum | 加快收敛 | 0.01-0.1 |
| Nesterov | 更新加快 | 0.01-0.1 |
| AdaGrad | 自适应学习率 | 0.001-0.01 |
| RMSprop | 自适应学习率 | 0.001-0.01 |
| Adam | 目前最流行 | 0.0001-0.001 |

---

## 第五部分：自然语言处理（NLP）

### NLP 任务概览

1. **文本分类** - 情感分析、垃圾检测
2. **命名实体识别** - 提取人名、地名等
3. **机器翻译** - 文本翻译
4. **问答系统** - 回答用户问题
5. **文本生成** - 生成新文本

### 文本表示方法

#### 1. One-Hot Encoding

每个词用一个独热向量表示。

**问题**：
- 维度爆炸
- 无法捕捉词义关系

#### 2. Word2Vec

学习词的低维向量表示。

**核心思想**：
- 上下文相似的词有相似的向量
- 关系可以通过向量运算表示

**例子**：
```
king - man + woman ≈ queen
```

#### 3. FastText

改进 Word2Vec，可处理未知词。

#### 4. Transformer Embeddings

使用预训练 Transformer 模型获得上下文相关的词向量。

**优势**：
- 相同词在不同上下文有不同表示
- 更好的语义理解

---

## 第六部分：实践建议

### 模型选择指南

| 任务 | 推荐模型 | 原因 |
|------|---------|------|
| 图像分类 | CNN（ResNet） | 特征提取能力强 |
| 文本分类 | BERT/RoBERTa | 预训练效果好 |
| 机器翻译 | Transformer | 并行化高效 |
| 时间序列 | LSTM/GRU | 内存能力强 |
| 通用NLP | GPT | 生成能力强 |

### 训练最佳实践

1. **数据准备** - 清洗、归一化、增强
2. **模型选择** - 从简单到复杂
3. **超参数调优** - 学习率、批大小等
4. **验证方法** - 交叉验证、留一验证
5. **监控指标** - 准确度、精确度、召回率、F1

### 常见陷阱

1. **数据泄漏** - 测试数据混入训练集
2. **调参过度** - 过度拟合超参数
3. **忽视基线** - 没有比较简单模型
4. **不平衡数据** - 类别不均等
5. **特征工程不足** - 特征质量差

---

## 总结

- 机器学习包括监督学习、无监督学习、强化学习
- 神经网络通过反向传播学习复杂的非线性映射
- 深度学习模型（CNN、RNN、Transformer）在各自领域表现优异
- 正则化和优化对模型训练至关重要
- 实践中要平衡模型复杂度和泛化能力
