# backend/knowledge_base.py

import os
import json
from typing import List, Dict, Optional
from pathlib import Path
import hashlib
from datetime import datetime

try:
    from langchain_community.document_loaders import PDFPlumberLoader, TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"Warning: langchain components not fully installed: {e}")
    LANGCHAIN_AVAILABLE = False


class LocalKnowledgeBase:
    """æœ¬åœ°çŸ¥è¯†åº“ç®¡ç†ç±»"""
    
    def __init__(self, 
                 db_path: str = "./knowledge_db",
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 openai_api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        Args:
            db_path: çŸ¥è¯†åº“æ•°æ®åº“è·¯å¾„
            chunk_size: æ–‡æœ¬å—å¤§å°
            chunk_overlap: æ–‡æœ¬å—é‡å 
            openai_api_key: OpenAI API Key (å¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–)
        """
        self.db_path = Path(db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.metadata_file = self.db_path / "metadata.json"
        
        # è·å– OpenAI API Key
        self.api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError(
                "âŒ OPENAI_API_KEY æœªè®¾ç½®ï¼\n"
                "   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=sk-..."
            )
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = None
        if LANGCHAIN_AVAILABLE:
            self.embeddings = self._init_embeddings()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_store = None
        if self.embeddings:
            self.load_vector_store()
        
        # åŠ è½½æ–‡ä»¶å…ƒæ•°æ®
        self.file_metadata = self._load_metadata()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ– OpenAI Embeddings"""
        try:
            print(f"ğŸ“¦ åˆå§‹åŒ– OpenAI Embeddings (æ¨¡å‹: text-embedding-3-small)...")
            
            # ğŸ”´ æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼Œå› ä¸º OpenAI ä¸æ”¯æŒ SOCKS ä»£ç†
            os.environ.pop('http_proxy', None)
            os.environ.pop('https_proxy', None)
            os.environ.pop('HTTP_PROXY', None)
            os.environ.pop('HTTPS_PROXY', None)
            os.environ.pop('all_proxy', None)
            os.environ.pop('ALL_PROXY', None)
            
            # è¯»å–è‡ªå®šä¹‰ API ç«¯ç‚¹
            api_base = os.getenv('OPENAI_BASE_URL')
            
            embeddings = OpenAIEmbeddings(
                api_key=self.api_key,
                model="text-embedding-3-small",
                api_base=api_base
            )
            print(f"âœ… OpenAI Embeddings åˆå§‹åŒ–æˆåŠŸï¼")
            if api_base:
                print(f"   API ç«¯ç‚¹: {api_base}")
            return embeddings
        except Exception as e:
            print(f"âŒ OpenAI Embeddings åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½æ–‡ä»¶å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½å…ƒæ•°æ®: {e}")
        return {}
    
    def _save_metadata(self):
        """ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜å…ƒæ•°æ®: {e}")
    
    def load_vector_store(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        faiss_path = self.db_path / "faiss_index"
        
        if faiss_path.exists() and self.embeddings:
            try:
                self.vector_store = FAISS.load_local(
                    str(faiss_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print(f"âœ… å‘é‡åº“å·²åŠ è½½: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            except Exception as e:
                print(f"âš ï¸ å‘é‡åº“åŠ è½½å¤±è´¥: {e}")
                self.vector_store = None
    
    def save_vector_store(self):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        if not self.vector_store:
            print(f"âš ï¸ å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return False
        
        faiss_path = self.db_path / "faiss_index"
        try:
            self.vector_store.save_local(str(faiss_path))
            print(f"âœ… å‘é‡åº“å·²ä¿å­˜: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            return True
        except Exception as e:
            print(f"âŒ å‘é‡åº“ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def add_documents(self, file_paths: List[str]) -> Dict:
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if not self.embeddings:
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [{'error': 'Embeddings æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£'}]
            }
        
        all_documents = []
        processed_files = {}
        errors = []
        
        print(f"\nğŸ“‚ å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        for file_path in file_paths:
            path = Path(file_path)
            
            if path.is_file():
                docs, error = self._load_file(path)
                if error:
                    errors.append(error)
                else:
                    all_documents.extend(docs)
                    processed_files[str(path)] = len(docs)
            elif path.is_dir():
                for ext in ['*.pdf', '*.txt', '*.md']:
                    for file_path in path.glob(f"**/{ext}"):
                        docs, error = self._load_file(file_path)
                        if error:
                            errors.append(error)
                        else:
                            all_documents.extend(docs)
                            processed_files[str(file_path)] = len(docs)
        
        if not all_documents:
            return {
                'added_chunks': 0,
                'files': [],
                'errors': errors
            }
        
        # åˆ†å‰²æ–‡æœ¬
        print(f"âœ‚ï¸ åˆ†å‰² {len(all_documents)} ä¸ªæ–‡æ¡£...")
        chunks = self._split_documents(all_documents)
        added_chunks = len(chunks)
        print(f"âœ… åˆ†å‰²å®Œæˆ: {added_chunks} ä¸ªå—")
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        try:
            if self.vector_store is None:
                print(f"ğŸ†• åˆ›å»ºæ–°å‘é‡åº“...")
                self.vector_store = FAISS.from_documents(chunks, self.embeddings)
            else:
                print(f"â• å‘ç°æœ‰å‘é‡åº“æ·»åŠ æ–‡æ¡£...")
                self.vector_store.add_documents(chunks)
            
            print(f"âœ… å‘é‡åº“æ›´æ–°æˆåŠŸ: ç°åœ¨å…± {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            
            # ä¿å­˜å‘é‡åº“
            self.save_vector_store()
        except Exception as e:
            print(f"âŒ æ·»åŠ åˆ°å‘é‡åº“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [{'error': f'å‘é‡åº“æ“ä½œå¤±è´¥: {e}'}]
            }
        
        # æ›´æ–°å…ƒæ•°æ®
        for file_path, doc_count in processed_files.items():
            file_name = Path(file_path).name
            self.file_metadata[file_name] = {
                'path': file_path,
                'hash': self._calculate_file_hash(file_path),
                'added_time': datetime.now().isoformat(),
                'doc_count': doc_count,
                'chunks': added_chunks
            }
        
        self._save_metadata()
        print(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜\n")
        
        return {
            'added_chunks': added_chunks,
            'files': list(processed_files.keys()),
            'errors': errors
        }
    
    def _load_file(self, file_path: Path) -> tuple:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        try:
            if file_path.suffix.lower() == '.pdf':
                loader = PDFPlumberLoader(str(file_path))
                docs = loader.load()
            elif file_path.suffix.lower() in ['.txt', '.md']:
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
            else:
                return [], {'file': str(file_path), 'error': f'ä¸æ”¯æŒçš„æ ¼å¼: {file_path.suffix}'}
            
            for doc in docs:
                doc.metadata['source'] = file_path.name
            
            print(f"  âœ… {file_path.name}: {len(docs)} ä¸ªæ–‡æ¡£")
            return docs, None
        
        except Exception as e:
            print(f"  âŒ {file_path.name}: {e}")
            return [], {'file': str(file_path), 'error': str(e)}
    
    def _split_documents(self, documents: List) -> List:
        """åˆ†å‰²æ–‡æ¡£"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
        )
        return splitter.split_documents(documents)
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return ""
    
    def search(self, query: str, top_k: int = 3) -> Dict:
        """æœç´¢çŸ¥è¯†åº“"""
        if not self.vector_store:
            return {'question': query, 'results': []}
        
        try:
            results = self.vector_store.similarity_search_with_score(query, k=top_k)
            documents = [
                {
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'Unknown'),
                    'score': float(score)
                }
                for doc, score in results
            ]
            return {'question': query, 'results': documents}
        except Exception as e:
            print(f"Search error: {e}")
            return {'question': query, 'results': []}
    
    def query(self, question: str, top_k: int = 3) -> Dict:
        """æŸ¥è¯¢çŸ¥è¯†åº“"""
        results = self.search(question, top_k)
        answer = "\n\n".join([
            f"ã€{doc['source']}ã€‘\n{doc['content']}"
            for doc in results['results']
        ])
        
        return {
            'question': question,
            'answer': answer or "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹",
            'sources': [doc['source'] for doc in results['results']]
        }
    
    def get_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            self.load_vector_store()
            
            total_chunks = self.vector_store.index.ntotal if self.vector_store else 0
            files = [
                {
                    'name': filename,
                    'path': metadata.get('path', ''),
                    'added_time': metadata.get('added_time', '')
                }
                for filename, metadata in self.file_metadata.items()
            ]
            
            return {
                'total_chunks': total_chunks,
                'total_files': len(files),
                'files': files
            }
        except Exception as e:
            print(f"Error getting stats: {e}")
            return {'total_chunks': 0, 'total_files': 0, 'files': []}
    
    def clear(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            import shutil
            if self.db_path.exists():
                shutil.rmtree(self.db_path)
                self.db_path.mkdir(parents=True, exist_ok=True)
            
            self.vector_store = None
            self.file_metadata = {}
            self._save_metadata()
            print("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")
    
    def delete_document(self, filename: str):
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        if filename in self.file_metadata:
            del self.file_metadata[filename]
            self._save_metadata()
            self.load_vector_store()
    
    def add_documents_from_upload(self, files) -> Dict:
        """ä»ä¸Šä¼ çš„æ–‡ä»¶æ·»åŠ æ–‡æ¡£"""
        import tempfile
        import shutil
        
        temp_dir = tempfile.mkdtemp()
        file_paths = []
        
        try:
            for file in files:
                temp_path = Path(temp_dir) / file.filename
                with open(temp_path, 'wb') as f:
                    f.write(file.file.read())
                file_paths.append(str(temp_path))
            
            result = self.add_documents(file_paths)
            
            # ä¿å­˜æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•
            for file_path in file_paths:
                path = Path(file_path)
                dest_path = self.db_path / "documents" / path.name
                dest_path.parent.mkdir(parents=True, exist_ok=True)
                shutil.copy2(file_path, dest_path)
                print(f"  ğŸ“„ æ–‡ä»¶å·²ä¿å­˜: {dest_path}")
            
            return result
        
        finally:
            shutil.rmtree(temp_dir, ignore_errors=True)
