# backend/knowledge_base.py

import os
import json
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import hashlib
from datetime import datetime


from dotenv import load_dotenv
# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œload_dotenv() ä¼šåœ¨å½“å‰ç›®å½•æŸ¥æ‰¾ .env æ–‡ä»¶
load_dotenv()

# åœ¨æœ€å¼€å§‹è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ # TODO å¦‚ä½•ä½“ç°æœ¬åœ°ä¼˜å…ˆ

project_root = Path(__file__).parent.parent  # é¡¹ç›®æ ¹ç›®å½•
models_cache_path = project_root / 'models_cache'

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_HOME'] = str(models_cache_path.absolute())
os.environ['TRANSFORMERS_CACHE'] = str((models_cache_path / 'transformers').absolute())

# V2Ray ä»£ç†åœ°å€ï¼‰
# os.environ["HTTP_PROXY"] = "http://127.0.0.1:10808"  # æµè§ˆå™¨ä»£ç†ç«¯å£
# os.environ["HTTPS_PROXY"] = "http://127.0.0.1:10808"  # æ³¨æ„ï¼šHTTPS ä»£ç†ä¹Ÿå¡« http å¼€å¤´ï¼ˆæœ¬åœ°ä»£ç†é€šç”¨ï¼‰
#

try:
    from langchain_community.document_loaders import PDFPlumberLoader, TextLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter  # âœ… æ”¹è¿™é‡Œ
    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"Warning: langchain components not fully installed: {e}")
    LANGCHAIN_AVAILABLE = False


class LocalKnowledgeBase:
    """æœ¬åœ°çŸ¥è¯†åº“ç®¡ç†ç±»"""
    
    def __init__(self, 
                 db_path: str = "./knowledge_db",
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 openai_api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        Args:
            db_path: çŸ¥è¯†åº“æ•°æ®åº“è·¯å¾„
            chunk_size: æ–‡æœ¬å—å¤§å°
            chunk_overlap: æ–‡æœ¬å—é‡å 
            openai_api_key: OpenAI API Key (å¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–)
        """

        self.db_path = Path(db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        # 1. åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½• - ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ models_cache
        # æ³¨æ„ï¼šè¿™ä¸ä¸Šé¢è®¾ç½®çš„ HF_HOME ç¯å¢ƒå˜é‡å¿…é¡»ä¸€è‡´
        project_root = Path(__file__).parent.parent
        self.models_cache = project_root / 'models_cache'
        self.models_cache.mkdir(parents=True, exist_ok=True)
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.metadata_file = self.db_path / "metadata.json"

        self.reranker = None
        self.reranker_model = 'light'
        
        # 2.æ”¹ä¸ºå»¶è¿ŸåŠ è½½ï¼šä¸åœ¨ __init__ ä¸­åŠ è½½æ¨¡å‹
        # è€Œæ˜¯åœ¨ search() æ–¹æ³•ä¸­ç¬¬ä¸€æ¬¡éœ€è¦æ—¶åŠ è½½
        # è¿™æ ·å¯ä»¥ç¡®ä¿ç¯å¢ƒå˜é‡å·²ç»æ­£ç¡®è®¾ç½®


                
        # 3. æ·»åŠ ç›¸å…³æ€§é˜ˆå€¼é…ç½®
        self.relevance_threshold = 0.3  # ç›¸å…³æ€§é˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼‰

        # 4. OpenAI API Key
        self.api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            self.api_key = openai_api_key

        if not self.api_key:
            raise ValueError(
                "âŒ OPENAI_API_KEY æœªè®¾ç½®ï¼\n"
                "   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=sk-..."
            )
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = None
        if LANGCHAIN_AVAILABLE:
            self.embeddings = self._init_embeddings()
            if not self.embeddings:
                print(f"âš ï¸ è­¦å‘Šï¼šEmbeddings åˆå§‹åŒ–å¤±è´¥ï¼ŒçŸ¥è¯†åº“åŠŸèƒ½å°†å—é™")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šLangChain ä¸å¯ç”¨ï¼ŒçŸ¥è¯†åº“åŠŸèƒ½å°†å—é™")
        
        # 6. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_store = None
        if self.embeddings:
            self.load_vector_store()
        
        # 7. åŠ è½½æ–‡ä»¶å…ƒæ•°æ®
        self.file_metadata = self._load_metadata()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ– OpenAI Embeddings"""
        try:
            print(f"ğŸ“¦ åˆå§‹åŒ– OpenAI Embeddings (æ¨¡å‹: text-embedding-3-small)...")
            
            # ğŸ”´ æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼Œå› ä¸º OpenAI ä¸æ”¯æŒ SOCKS ä»£ç†
            os.environ.pop('http_proxy', None)
            os.environ.pop('https_proxy', None)
            os.environ.pop('HTTP_PROXY', None)
            os.environ.pop('HTTPS_PROXY', None)
            os.environ.pop('all_proxy', None)
            os.environ.pop('ALL_PROXY', None)
            
            # è¯»å–è‡ªå®šä¹‰ API ç«¯ç‚¹
            api_base = os.getenv('OPENAI_BASE_URL')

            embeddings = OpenAIEmbeddings(
                api_key=self.api_key,
                model="text-embedding-3-small",
                base_url=api_base
            )
            print(f"âœ… OpenAI Embeddings åˆå§‹åŒ–æˆåŠŸï¼")
            if api_base:
                print(f"   API ç«¯ç‚¹: {api_base}")
            return embeddings
        except Exception as e:
            print(f"âŒ OpenAI Embeddings åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½æ–‡ä»¶å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½å…ƒæ•°æ®: {e}")
        return {}
    
    def _save_metadata(self):
        """ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜å…ƒæ•°æ®: {e}")
    
    def _clean_filename(self, filename: str) -> str:
        """æ¸…ç†æ–‡ä»¶åå‰ç¼€ï¼ˆå»æ‰å¦‚ '0_' æˆ– '123_' çš„å‰ç¼€ï¼‰"""
        if '_' in filename:
            parts = filename.split('_', 1)
            if len(parts) == 2 and parts[0].isdigit():
                return parts[1]
        return filename
    
    def load_vector_store(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        faiss_path = self.db_path / "faiss_index"
        
        if faiss_path.exists() and self.embeddings:
            try:
                self.vector_store = FAISS.load_local(
                    str(faiss_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print(f"âœ… å‘é‡åº“å·²åŠ è½½: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            except Exception as e:
                print(f"âš ï¸ å‘é‡åº“åŠ è½½å¤±è´¥: {e}")
                self.vector_store = None
    
    def save_vector_store(self):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        if not self.vector_store:
            print(f"âš ï¸ å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return False
        
        faiss_path = self.db_path / "faiss_index"
        try:
            self.vector_store.save_local(str(faiss_path))
            print(f"âœ… å‘é‡åº“å·²ä¿å­˜: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            return True
        except Exception as e:
            print(f"âŒ å‘é‡åº“ä¿å­˜å¤±è´¥: {e}")
            return False

    def _rebuild_vector_store(self):
        """æ ¹æ®å½“å‰å…ƒæ•°æ®é‡å»ºå‘é‡ç´¢å¼•ï¼ˆä»ç£ç›˜æ–‡ä»¶åŠ è½½æ‰€æœ‰æ–‡æ¡£å¹¶é‡å»º FAISSï¼‰ã€‚
        è¯´æ˜ï¼šè¯¥æ–¹æ³•åªé‡å»ºå‘é‡ç´¢å¼•ï¼Œä¸ä¼šä¿®æ”¹ `file_metadata` çš„æ—¶é—´ç­‰å­—æ®µã€‚
        """
        if not self.embeddings:
            print("âš ï¸ Embeddings æœªåˆå§‹åŒ–ï¼Œæ— æ³•é‡å»ºç´¢å¼•")
            return False

        # æ”¶é›†æ‰€æœ‰æ–‡ä»¶è·¯å¾„
        file_paths = []
        for fname, meta in self.file_metadata.items():
            path = meta.get('path')
            if path:
                p = Path(path)
                if p.exists():
                    file_paths.append(str(p))

        if not file_paths:
            print("âš ï¸ æœªæ‰¾åˆ°å¯ç”¨äºé‡å»ºçš„æ–‡æ¡£æ–‡ä»¶ï¼Œæ¸…ç©ºå‘é‡åº“")
            self.vector_store = None
            # åˆ é™¤å·²å­˜åœ¨çš„ faiss_index ç›®å½•ä»¥é¿å…ä¸ä¸€è‡´
            try:
                faiss_path = self.db_path / "faiss_index"
                if faiss_path.exists():
                    import shutil
                    shutil.rmtree(str(faiss_path))
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤æ—§å‘é‡åº“å¤±è´¥: {e}")
            return True

        try:
            print(f"ğŸ”§ é‡å»ºå‘é‡åº“ï¼šå°†ä» {len(file_paths)} ä¸ªæ–‡ä»¶åˆ›å»ºç´¢å¼•...")

            all_documents = []
            for fp in file_paths:
                try:
                    docs, err = self._load_file(Path(fp))
                    if err:
                        print(f"  âš ï¸ åŠ è½½æ–‡æ¡£å¤±è´¥: {fp} -> {err}")
                        continue
                    all_documents.extend(docs)
                except Exception as e:
                    print(f"  âŒ è¯»å–æ–‡ä»¶ {fp} å¤±è´¥: {e}")

            if not all_documents:
                print("âš ï¸ æ²¡æœ‰å¯ç”¨æ–‡æ¡£å†…å®¹æ¥é‡å»ºç´¢å¼•")
                self.vector_store = None
                return True

            # åˆ†å‰²æ–‡æ¡£
            splitter = RecursiveCharacterTextSplitter(
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
            split_docs = splitter.split_documents(all_documents)
            print(f"âœ… åˆ†å‰²å®Œæˆï¼Œå…± {len(split_docs)} ä¸ª chunksï¼Œå¼€å§‹åˆ›å»º/æ›¿æ¢ FAISS ç´¢å¼•...")

            # ä½¿ç”¨ FAISS.from_documents é‡æ–°åˆ›å»ºç´¢å¼•
            try:
                self.vector_store = FAISS.from_documents(split_docs, self.embeddings)
                # ä¿å­˜åˆ°ç£ç›˜
                self.save_vector_store()
                print(f"âœ… å‘é‡åº“é‡å»ºå®Œæˆ: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
                return True
            except Exception as e:
                print(f"âŒ åˆ›å»ºå‘é‡åº“å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                return False

        except Exception as e:
            print(f"âŒ é‡å»ºå‘é‡åº“é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def add_documents(self, file_paths: List[str], progress_callback=None) -> Dict:
        """æ·»åŠ æ–‡æ¡£ - æ”¯æŒè¿›åº¦å›è°ƒ"""
        print(f"\nğŸ“‚ å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        if not self.embeddings:
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [{'error': 'Embeddings æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£'}]
            }
        
        all_documents = []
        processed_files = {}
        added_chunks = 0
        errors = []
        total_files = len(file_paths)
        
        # ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ‰€æœ‰æ–‡æ¡£
        print("\nğŸ“– ç¬¬ä¸€æ­¥ï¼šåŠ è½½æ–‡æ¡£...")
        for idx, file_path in enumerate(file_paths):
            try:
                docs, error = self._load_file(Path(file_path))
                if error:
                    errors.append(error)
                else:
                    all_documents.extend(docs)
                    processed_files[file_path] = len(docs)
                    
                    # ğŸ“¤ å‘é€åŠ è½½è¿›åº¦ï¼ˆ0-40%ï¼‰
                    progress = int((idx + 1) / total_files * 40)
                    if progress_callback:
                        progress_callback('loading', progress)
            
            except Exception as e:
                print(f"âŒ åŠ è½½æ–‡ä»¶å¤±è´¥: {file_path}, {e}")
                errors.append({'file': str(file_path), 'error': str(e)})
        
        if not all_documents:
            print("âš ï¸ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æ¡£")
            return {
                'added_chunks': 0,
                'files': [],
                'errors': errors
            }
        
        # ç¬¬äºŒæ­¥ï¼šåˆ†å‰²æ–‡æ¡£
        print("\nâœ‚ï¸ ç¬¬äºŒæ­¥ï¼šåˆ†å‰²æ–‡æ¡£...")
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap
        )
        split_docs = splitter.split_documents(all_documents)
        print(f"âœ… åˆ†å‰²å®Œæˆï¼Œå…± {len(split_docs)} ä¸ª chunks")
        # ===== ä¿å­˜åˆ†å—å†…å®¹åˆ°å…ƒæ•°æ®ï¼ˆæŒ‰æ–‡ä»¶åˆ†ç»„ï¼‰ =====
        try:
            chunks_by_file = {}
            for idx, doc in enumerate(split_docs):
                source = doc.metadata.get('source', 'unknown')
                entry = {
                    'id': idx,
                    'content': doc.page_content[:2000]  # ä¿å­˜å‰2kå­—ç¬¦ç”¨äºé¢„è§ˆ
                }
                chunks_by_file.setdefault(source, []).append(entry)

            # å°†åˆ†å—è¯¦æƒ…åˆå¹¶åˆ° file_metadata ä¸­
            for file_path, doc_count in processed_files.items():
                file_name = self._clean_filename(Path(file_path).name)
                if file_name in chunks_by_file:
                    self.file_metadata.setdefault(file_name, {})
                    self.file_metadata[file_name]['chunks_detail'] = chunks_by_file[file_name]
                    # ensure chunks count recorded
                    self.file_metadata[file_name]['chunks'] = len(chunks_by_file[file_name])
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜åˆ†å—è¯¦æƒ…å¤±è´¥: {e}")
        
        # ğŸ“¤ å‘é€åˆ†å‰²è¿›åº¦ï¼ˆ40-60%ï¼‰
        if progress_callback:
            progress_callback('splitting', 60)
        
        # ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå‘é‡ï¼ˆè¿™æ˜¯æœ€è€—æ—¶çš„æ­¥éª¤ï¼‰
        print("\nğŸ”¢ ç¬¬ä¸‰æ­¥ï¼šç”Ÿæˆå‘é‡ï¼ˆè¿™å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼‰...")
        total_chunks = len(split_docs)
        
        try:
            # æ‰¹é‡å¤„ç† chunksï¼Œæ¯æ‰¹ 10 ä¸ª
            batch_size = 10
            for batch_idx in range(0, len(split_docs), batch_size):
                batch = split_docs[batch_idx:batch_idx + batch_size]
                
                try:
                    if self.vector_store is None:
                        # ç¬¬ä¸€æ‰¹ï¼šåˆ›å»ºå‘é‡åº“
                        self.vector_store = FAISS.from_documents(batch, self.embeddings)
                    else:
                        # åç»­æ‰¹ï¼šæ·»åŠ åˆ°ç°æœ‰å‘é‡åº“
                        self.vector_store.add_documents(batch)
                    
                    added_chunks += len(batch)
                    
                    # ğŸ“¤ å‘é€å‘é‡åŒ–è¿›åº¦ï¼ˆ60-95%ï¼‰
                    progress = 60 + int((batch_idx + len(batch)) / total_chunks * 35)
                    if progress_callback:
                        progress_callback('vectorizing', min(progress, 95))
                    
                    print(f"âœ… å¤„ç†äº† {added_chunks}/{total_chunks} chunks")
                
                except Exception as e:
                    print(f"âŒ å‘é‡åŒ–å¤±è´¥: {e}")
                    errors.append({'error': f'å‘é‡åŒ–å¤±è´¥: {e}'})
                    return {
                        'added_chunks': added_chunks,
                        'files': list(processed_files.keys()),
                        'errors': errors
                    }
            
            # ç¬¬å››æ­¥ï¼šä¿å­˜å‘é‡åº“
            print("\nğŸ’¾ ç¬¬å››æ­¥ï¼šä¿å­˜å‘é‡åº“...")
            self.save_vector_store()
            
            # ğŸ“¤ å‘é€ä¿å­˜è¿›åº¦ï¼ˆ95-100%ï¼‰
            if progress_callback:
                progress_callback('saving', 100)
            
            # æ›´æ–°å…ƒæ•°æ®
            for file_path, doc_count in processed_files.items():
                file_name = self._clean_filename(Path(file_path).name)
                # ä¸è¦è¦†ç›–å·²æœ‰ metadataï¼ˆä¾‹å¦‚ chunks_detailï¼‰ï¼Œè€Œæ˜¯æ›´æ–°å­—æ®µ
                self.file_metadata.setdefault(file_name, {})
                # å¦‚æœä¹‹å‰å·²ç»è®¡ç®—äº†åˆ†å—è¯¦æƒ…ï¼Œåˆ™ä¼˜å…ˆä½¿ç”¨å…¶é•¿åº¦ä½œä¸º chunks
                existing_chunks = self.file_metadata[file_name].get('chunks')
                if existing_chunks is None:
                    # å¦‚æœæ²¡æœ‰ï¼Œå°è¯•ä½¿ç”¨ chunks_detail é•¿åº¦
                    existing_chunks = len(self.file_metadata[file_name].get('chunks_detail', [])) or doc_count

                self.file_metadata[file_name].update({
                    'path': file_path,
                    'hash': self._calculate_file_hash(file_path),
                    'added_time': datetime.now().isoformat(),
                    'chunks': existing_chunks,
                    'size': Path(file_path).stat().st_size if Path(file_path).exists() else None,
                    'status': 'indexed'
                })
            
            self._save_metadata()
            
            print(f"âœ… å®Œæˆï¼å…±æ·»åŠ  {added_chunks} ä¸ª chunks\n")
            
            return {
                'added_chunks': added_chunks,
                'files': list(processed_files.keys()),
                'errors': errors
            }
        
        except Exception as e:
            print(f"âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'added_chunks': added_chunks,
                'files': list(processed_files.keys()),
                'errors': [{'error': f'å¤„ç†å¤±è´¥: {e}'}]
            }
    
    def _load_file(self, file_path: Path) -> tuple:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        try:
            if file_path.suffix.lower() == '.pdf':
                loader = PDFPlumberLoader(str(file_path))
                docs = loader.load()
            elif file_path.suffix.lower() in ['.txt', '.md']:
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
            else:
                return [], {'file': str(file_path), 'error': f'ä¸æ”¯æŒçš„æ ¼å¼: {file_path.suffix}'}
            
            for doc in docs:
                doc.metadata['source'] = self._clean_filename(file_path.name)
            
            print(f"  âœ… {file_path.name}: {len(docs)} ä¸ªæ–‡æ¡£")
            return docs, None
        
        except Exception as e:
            print(f"  âŒ {file_path.name}: {e}")
            return [], {'file': str(file_path), 'error': str(e)}
    
    def _split_documents(self, documents: List) -> List:
        """åˆ†å‰²æ–‡æ¡£"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
        )
        return splitter.split_documents(documents)
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return ""
    
    def search(self, query: str, top_k: int = 3, use_reranking: bool = True) -> Dict:
        """
        æœç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒé‡æ’åºï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„ç»“æœæ•°
            use_reranking: æ˜¯å¦ä½¿ç”¨é‡æ’åºå™¨
        """
        if not self.vector_store:
            print(f"çŸ¥è¯†åº“ä¸å­˜åœ¨æˆ–æœªåŠ è½½")
            return {'question': query, 'results': [], 'has_results': False}
        else:
            print(f"ğŸ” å¼€å§‹æœç´¢: '{query}' (Top {top_k}, é‡æ’åº: {'å¯ç”¨' if use_reranking else 'ç¦ç”¨'})")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šå‘é‡æ£€ç´¢ï¼ˆå¬å›æ›´å¤šå€™é€‰ï¼‰
            candidates = self.vector_store.similarity_search_with_score(
                query, 
                k=top_k * 3  # å¬å› 3 å€çš„å€™é€‰
            )

            # ä½¿ç”¨æä¾›çš„é˜ˆå€¼æˆ–é»˜è®¤å€¼
            threshold = self.relevance_threshold

            # è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼Œæ‰€ä»¥è¦ç”¨ 1 / (1 + distance) è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            filtered_candidates = []
            for doc, distance in candidates:
                # âœ… æ­£ç¡®çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šè·ç¦» â†’ ç›¸ä¼¼åº¦
                # distance èŒƒå›´ï¼š[0, âˆ)
                # similarity èŒƒå›´ï¼š(0, 1]
                # ä½¿ç”¨å…¬å¼ï¼šsimilarity = 1 / (1 + distance)
                similarity = 1 / (1 + distance)
                
                source_name = doc.metadata.get('source', 'Unknown')
                print(f"ğŸ“Š æœç´¢ç»“æœ: {source_name} (è·ç¦»: {distance:.3f}, ç›¸ä¼¼åº¦: {similarity:.3f})")
                
                # âœ… æŒ‰ç›¸å…³æ€§é˜ˆå€¼è¿‡æ»¤
                if similarity >= threshold:
                    filtered_candidates.append({
                        'content': doc.page_content, # æ–‡æ¡£å†…å®¹
                        'source': source_name, # æ–‡æ¡£æ¥æº
                        'score': similarity, # ä½¿ç”¨ç›¸ä¼¼åº¦ä½œä¸ºåˆ†æ•°
                        'distance': distance  # ä¿ç•™åŸå§‹è·ç¦»ç”¨äºè°ƒè¯•
                    })
                else:
                    print(f"   âŒ ç›¸ä¼¼åº¦è¿‡ä½ï¼Œè¿‡æ»¤æ‰")

            # âœ… åªè¿”å› top_k ä¸ªç»“æœ
            filtered_candidates = filtered_candidates[:top_k]
            
            # ç¬¬äºŒæ­¥ï¼šé‡æ’åº
            if use_reranking:
                try:
                    # âœ… å»¶è¿Ÿå¯¼å…¥ï¼šåœ¨ä½¿ç”¨æ—¶æ‰å¯¼å…¥
                    from sentence_transformers import CrossEncoder
                    
                    # âœ… ç¬¬ä¸€æ¬¡éœ€è¦æ—¶æ‰åŠ è½½æ¨¡å‹
                    if self.reranker is None:
                        # å®šä¹‰æ¨¡å‹æ˜ å°„
                        model_map = {
                            'light': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
                            'medium': 'BAAI/bge-reranker-base',
                            'large': 'BAAI/bge-reranker-large'
                        }
                        
                        model_name = model_map.get(self.reranker_model, model_map['light'])
                        
                        try:
                            print(f"ğŸ“¦ [å»¶è¿ŸåŠ è½½] åŠ è½½é‡æ’åºæ¨¡å‹: {model_name}...")
                            # âœ… æ˜ç¡®æŒ‡å®šç¼“å­˜ç›®å½•
                            cache_folder = str(self.models_cache.absolute())
                            
                            self.reranker = CrossEncoder(
                                model_name,
                                cache_folder=cache_folder  # âœ… æŒ‡å®šç¼“å­˜ä½ç½®
                            )
                            print(f"âœ… é‡æ’åºå™¨åŠ è½½æˆåŠŸ (ç¼“å­˜: {cache_folder})")
                        except Exception as load_error:
                            print(f"âš ï¸ é‡æ’åºå™¨åŠ è½½å¤±è´¥: {load_error}")
                            print(f"   ä½¿ç”¨åŸå§‹å‘é‡æœç´¢")
                            self.reranker = None
                    
                    # âœ… åªæœ‰æ¨¡å‹åŠ è½½æˆåŠŸæ‰æ‰§è¡Œé‡æ’åº
                    if self.reranker is not None:
                        # æå–æ–‡æ¡£å†…å®¹ï¼ˆä»å­—å…¸ä¸­è·å–ï¼‰
                        doc_contents = [cand['content'] for cand in filtered_candidates]
                        
                        # é‡æ’åº
                        scores = self.reranker.predict([
                            (query, content) for content in doc_contents
                        ])
                        
                        # ç»„åˆå€™é€‰æ–‡æ¡£å’Œåˆ†æ•°ï¼Œå¹¶æ’åº
                        ranked_pairs = list(zip(filtered_candidates, scores))
                        ranked_pairs.sort(key=lambda x: x[1], reverse=True)
                        
                        # æ›´æ–°ä¸ºæ’åºåçš„ç»“æœ
                        candidates = ranked_pairs
                        print(f"âœ… é‡æ’åºå®Œæˆ: {len(candidates)} ä¸ªç»“æœ")
                    
                except Exception as e:
                    print(f"âš ï¸  Re-Ranking å¤±è´¥ï¼Œé™çº§åˆ°å‘é‡ç›¸ä¼¼åº¦: {e}")
                    # é™çº§å¤„ç†ï¼šç»§ç»­ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
                    pass
            
            # ç¬¬ä¸‰æ­¥ï¼šæ ¼å¼åŒ–ç»“æœï¼ˆä¸å†éœ€è¦ç¡¬é˜ˆå€¼ï¼ï¼‰
            results = []
            for doc, score in candidates[:top_k]:
                results.append({
                    'content': doc.get('content') if isinstance(doc, dict) else doc.page_content,
                    'source': doc.get('source') if isinstance(doc, dict) else doc.metadata.get('source', 'Unknown'),
                    'score': float(score),  # ç°åœ¨æ˜¯é‡æ’åºåˆ†æ•°è€Œä¸æ˜¯å‘é‡è·ç¦»
                })
            
            has_results = len(results) > 0
            
            print(f"âœ… æœç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result['source']} (åˆ†æ•°: {result['score']:.3f})")
            
            return {
                'question': query,
                'results': results,
                'has_results': has_results
            }
        
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {
                'question': query,
                'results': [],
                'has_results': False
            }

    def query(self, question: str, top_k: int = 3) -> Dict:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“
        
        Returns:
            {
                'question': str,
                'answer': str,
                'sources': list,
                'has_sources': bool  # âœ… æ–°å¢å­—æ®µï¼Œè¡¨ç¤ºæ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            }
        """
        search_results = self.search(question, top_k)
        results = search_results['results']
        has_sources = search_results['has_results']
        
        answer = "\n\n".join([
            f"ã€{doc['source']}ã€‘\n{doc['content']}"
            for doc in results
        ])
        
        sources = [doc['source'] for doc in results]
        
        # âœ… å»é‡ sources
        sources = list(dict.fromkeys(sources))
        
        return {
            'question': question,
            'answer': answer or "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹",
            'sources': sources,
            'has_sources': has_sources  # âœ… æ–°å¢ï¼šæ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
        }
    
    def get_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            self.load_vector_store()
            
            total_chunks = self.vector_store.index.ntotal if self.vector_store else 0
            files = [
                {
                    'name': filename,
                    'path': metadata.get('path', ''),
                    'added_time': metadata.get('added_time', ''),
                    'upload_time': metadata.get('added_time', ''),
                    'size': metadata.get('size'),
                    'chunks': metadata.get('chunks') or metadata.get('doc_count') or 0,
                    'status': metadata.get('status', 'unknown')
                }
                for filename, metadata in self.file_metadata.items()
            ]
            
            return {
                'total_chunks': total_chunks,
                'total_files': len(files),
                'files': files
            }
        except Exception as e:
            print(f"Error getting stats: {e}")
            return {'total_chunks': 0, 'total_files': 0, 'files': []}
    
    def clear(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            import shutil
            if self.db_path.exists():
                shutil.rmtree(self.db_path)
                self.db_path.mkdir(parents=True, exist_ok=True)
            
            self.vector_store = None
            self.file_metadata = {}
            self._save_metadata()
            print("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")
    
    def delete_document(self, filename: str):
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        if filename in self.file_metadata:
            # å°è¯•åˆ é™¤ç‰©ç†æ–‡ä»¶
            try:
                path = Path(self.file_metadata[filename].get('path', ''))
                if path.exists():
                    path.unlink()
                    # å¦‚æœæ‰€åœ¨ç›®å½•å˜ç©ºå¯é€‰æ‹©åˆ é™¤ç›®å½•ï¼Œä½†è¿™é‡Œä¸åšé¢å¤–åˆ é™¤
            except Exception as e:
                print(f"âš ï¸ åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {e}")

            # ä»å…ƒæ•°æ®ä¸­ç§»é™¤å¹¶ä¿å­˜
            del self.file_metadata[filename]
            self._save_metadata()

            # é‡æ–°æ„å»ºå‘é‡åº“ä»¥ç§»é™¤è¯¥æ–‡æ¡£çš„å‘é‡ï¼ˆè¾ƒé‡ï¼Œä½†ç¡®ä¿ç´¢å¼•ä¸€è‡´ï¼‰
            rebuilt = self._rebuild_vector_store()
            if not rebuilt:
                print("âš ï¸ é‡å»ºå‘é‡åº“å¤±è´¥ï¼Œå°è¯•åŠ è½½åŸæœ‰å‘é‡åº“")
                self.load_vector_store()
    
    def add_documents_from_upload(self, files) -> Dict:
        """ä»ä¸Šä¼ çš„æ–‡ä»¶æ·»åŠ æ–‡æ¡£"""
        import tempfile
        import shutil
        from pathlib import Path
        
        temp_dir = tempfile.mkdtemp()
        file_paths = []
        processed_files = []
        
        try:
            print(f"\nğŸ“ å¼€å§‹å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå…± {len(files)} ä¸ª")
            
            for idx, file in enumerate(files):
                try:
                    filename = file.filename
                    if not filename:
                        print(f"  âš ï¸  æ–‡ä»¶ {idx+1} æ²¡æœ‰æ–‡ä»¶åï¼Œè·³è¿‡")
                        continue
                    
                    print(f"  å¤„ç†æ–‡ä»¶ {idx+1}: {filename}")
                    
                    # âœ… ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¿å­˜æ–‡ä»¶
                    temp_path = Path(temp_dir) / filename
                    file.save(str(temp_path))
                    file_paths.append(str(temp_path))
                    processed_files.append(filename)
                    print(f"    âœ… å·²ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•")
                    
                except Exception as e:
                    print(f"  âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
                    continue
            
            if not file_paths:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯ä»¥å¤„ç†")
                return {
                    'added_chunks': 0,
                    'files': [],
                    'errors': ['æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶']
                }
            
            print(f"\nğŸ“š å¼€å§‹å¤„ç†æ–‡æ¡£å‘é‡åŒ–ï¼ˆ{len(file_paths)} ä¸ªæ–‡ä»¶ï¼‰...")
            result = self.add_documents(file_paths)
            
            # ä¿å­˜æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•
            print(f"\nğŸ’¾ ä¿å­˜æ–‡ä»¶åˆ°çŸ¥è¯†åº“...")
            for file_path in file_paths:
                try:
                    path = Path(file_path)
                    dest_path = self.db_path / "documents" / path.name
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, dest_path)
                    print(f"  âœ… {path.name}")
                    # æ›´æ–°å…ƒæ•°æ®ä¸­çš„è·¯å¾„å’Œå¤§å°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    clean_name = self._clean_filename(path.name)
                    if clean_name in self.file_metadata:
                        try:
                            self.file_metadata[clean_name]['path'] = str(dest_path)
                            self.file_metadata[clean_name]['size'] = dest_path.stat().st_size
                            # ä¿æŒ status ä¸º indexedï¼ˆå¦‚æœä¹‹å‰å·²è®¾ç½®ï¼‰
                        except Exception as e:
                            print(f"  âš ï¸ æ›´æ–°å…ƒæ•°æ®å¤§å°/è·¯å¾„å¤±è´¥: {e}")
                except Exception as e:
                    print(f"  âš ï¸  ä¿å­˜å¤±è´¥: {e}")

            # ä¿å­˜æ›´æ–°åçš„å…ƒæ•°æ®
            try:
                self._save_metadata()
            except Exception as e:
                print(f"âš ï¸ ä¿å­˜å…ƒæ•°æ®å¤±è´¥: {e}")
            
            print(f"\nâœ… ä¸Šä¼ å®Œæˆ!\n")
            return result
        
        except Exception as e:
            print(f"\nâŒ ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [str(e)]
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass
