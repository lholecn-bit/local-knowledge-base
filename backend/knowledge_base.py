# backend/knowledge_base.py

import os
import json
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import hashlib
from datetime import datetime

from dotenv import load_dotenv
# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œload_dotenv() ä¼šåœ¨å½“å‰ç›®å½•æŸ¥æ‰¾ .env æ–‡ä»¶
load_dotenv()

# ğŸ”¥ å…³é”®ï¼šåœ¨æœ€å¼€å§‹è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜
# ä½¿ç”¨ç»å¯¹è·¯å¾„é¿å…ç›¸å¯¹è·¯å¾„æ··ä¹±
project_root = Path(__file__).parent.parent  # é¡¹ç›®æ ¹ç›®å½•
models_cache_path = project_root / 'models_cache'

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_HOME'] = str(models_cache_path.absolute())
os.environ['TRANSFORMERS_CACHE'] = str((models_cache_path / 'transformers').absolute())



try:
    from langchain_community.document_loaders import PDFPlumberLoader, TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"Warning: langchain components not fully installed: {e}")
    LANGCHAIN_AVAILABLE = False


class LocalKnowledgeBase:
    """æœ¬åœ°çŸ¥è¯†åº“ç®¡ç†ç±»"""
    
    def __init__(self, 
                 db_path: str = "./knowledge_db",
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 openai_api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        Args:
            db_path: çŸ¥è¯†åº“æ•°æ®åº“è·¯å¾„
            chunk_size: æ–‡æœ¬å—å¤§å°
            chunk_overlap: æ–‡æœ¬å—é‡å 
            openai_api_key: OpenAI API Key (å¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–)
        """
        self.db_path = Path(db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        # âœ… åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½• - ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ models_cache
        # æ³¨æ„ï¼šè¿™ä¸ä¸Šé¢è®¾ç½®çš„ HF_HOME ç¯å¢ƒå˜é‡å¿…é¡»ä¸€è‡´
        project_root = Path(__file__).parent.parent
        self.models_cache = project_root / 'models_cache'
        self.models_cache.mkdir(parents=True, exist_ok=True)
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.metadata_file = self.db_path / "metadata.json"
        
        # âœ… æ·»åŠ ç›¸å…³æ€§é˜ˆå€¼é…ç½®
        self.relevance_threshold = 0.3  # ç›¸å…³æ€§é˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼‰

        # è·å– OpenAI API Key
        self.api_key = os.getenv("OPENAI_API_KEY")

        if openai_api_key:
            self.api_key = openai_api_key        

        if not self.api_key:
            raise ValueError(
                "âŒ OPENAI_API_KEY æœªè®¾ç½®ï¼\n"
                "   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=sk-..."
            )
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = None
        if LANGCHAIN_AVAILABLE:
            self.embeddings = self._init_embeddings()
        
        # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_store = None
        if self.embeddings:
            self.load_vector_store()
        
        # åŠ è½½æ–‡ä»¶å…ƒæ•°æ®
        self.file_metadata = self._load_metadata()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ– OpenAI Embeddings"""
        try:
            print(f"ğŸ“¦ åˆå§‹åŒ– OpenAI Embeddings (æ¨¡å‹: text-embedding-3-small)...")
            
            # ğŸ”´ æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼Œå› ä¸º OpenAI ä¸æ”¯æŒ SOCKS ä»£ç†
            os.environ.pop('http_proxy', None)
            os.environ.pop('https_proxy', None)
            os.environ.pop('HTTP_PROXY', None)
            os.environ.pop('HTTPS_PROXY', None)
            os.environ.pop('all_proxy', None)
            os.environ.pop('ALL_PROXY', None)
            
            # è¯»å–è‡ªå®šä¹‰ API ç«¯ç‚¹
            api_base = os.getenv('OPENAI_BASE_URL')

            embeddings = OpenAIEmbeddings(
                api_key=self.api_key,
                model="text-embedding-3-small",
                base_url=api_base
            )
            print(f"âœ… OpenAI Embeddings åˆå§‹åŒ–æˆåŠŸï¼")
            if api_base:
                print(f"   API ç«¯ç‚¹: {api_base}")
            return embeddings
        except Exception as e:
            print(f"âŒ OpenAI Embeddings åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½æ–‡ä»¶å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½å…ƒæ•°æ®: {e}")
        return {}
    
    def _save_metadata(self):
        """ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜å…ƒæ•°æ®: {e}")
    
    def load_vector_store(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        faiss_path = self.db_path / "faiss_index"
        
        if faiss_path.exists() and self.embeddings:
            try:
                self.vector_store = FAISS.load_local(
                    str(faiss_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print(f"âœ… å‘é‡åº“å·²åŠ è½½: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            except Exception as e:
                print(f"âš ï¸ å‘é‡åº“åŠ è½½å¤±è´¥: {e}")
                self.vector_store = None
    
    def save_vector_store(self):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        if not self.vector_store:
            print(f"âš ï¸ å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return False
        
        faiss_path = self.db_path / "faiss_index"
        try:
            self.vector_store.save_local(str(faiss_path))
            print(f"âœ… å‘é‡åº“å·²ä¿å­˜: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            return True
        except Exception as e:
            print(f"âŒ å‘é‡åº“ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def add_documents(self, file_paths: List[str]) -> Dict:
        print(f"\nğŸ“‚ å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if not self.embeddings:
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [{'error': 'Embeddings æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£'}]
            }
        
        all_documents = []
        processed_files = {}
        errors = []
        
        print(f"\nğŸ“‚ å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        for file_path in file_paths:
            path = Path(file_path)
            
            if path.is_file():
                docs, error = self._load_file(path)
                if error:
                    errors.append(error)
                else:
                    all_documents.extend(docs)
                    processed_files[str(path)] = len(docs)
            elif path.is_dir():
                for ext in ['*.pdf', '*.txt', '*.md']:
                    for file_path in path.glob(f"**/{ext}"):
                        docs, error = self._load_file(file_path)
                        if error:
                            errors.append(error)
                        else:
                            all_documents.extend(docs)
                            processed_files[str(file_path)] = len(docs)
        
        if not all_documents:
            return {
                'added_chunks': 0,
                'files': [],
                'errors': errors
            }
        
        # åˆ†å‰²æ–‡æœ¬
        print(f"âœ‚ï¸ åˆ†å‰² {len(all_documents)} ä¸ªæ–‡æ¡£...")
        chunks = self._split_documents(all_documents)
        added_chunks = len(chunks)
        print(f"âœ… åˆ†å‰²å®Œæˆ: {added_chunks} ä¸ªå—")
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        try:
            if self.vector_store is None:
                print(f"ğŸ†• åˆ›å»ºæ–°å‘é‡åº“...")
                self.vector_store = FAISS.from_documents(chunks, self.embeddings)
            else:
                print(f"â• å‘ç°æœ‰å‘é‡åº“æ·»åŠ æ–‡æ¡£...")
                self.vector_store.add_documents(chunks)
            
            print(f"âœ… å‘é‡åº“æ›´æ–°æˆåŠŸ: ç°åœ¨å…± {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            
            # ä¿å­˜å‘é‡åº“
            self.save_vector_store()
        except Exception as e:
            print(f"âŒ æ·»åŠ åˆ°å‘é‡åº“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [{'error': f'å‘é‡åº“æ“ä½œå¤±è´¥: {e}'}]
            }
        
        # æ›´æ–°å…ƒæ•°æ®
        for file_path, doc_count in processed_files.items():
            file_name = Path(file_path).name
            self.file_metadata[file_name] = {
                'path': file_path,
                'hash': self._calculate_file_hash(file_path),
                'added_time': datetime.now().isoformat(),
                'doc_count': doc_count,
                'chunks': added_chunks
            }
        
        self._save_metadata()
        print(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜\n")
        
        return {
            'added_chunks': added_chunks,
            'files': list(processed_files.keys()),
            'errors': errors
        }
    
    def _load_file(self, file_path: Path) -> tuple:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        try:
            if file_path.suffix.lower() == '.pdf':
                loader = PDFPlumberLoader(str(file_path))
                docs = loader.load()
            elif file_path.suffix.lower() in ['.txt', '.md']:
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
            else:
                return [], {'file': str(file_path), 'error': f'ä¸æ”¯æŒçš„æ ¼å¼: {file_path.suffix}'}
            
            for doc in docs:
                doc.metadata['source'] = file_path.name
            
            print(f"  âœ… {file_path.name}: {len(docs)} ä¸ªæ–‡æ¡£")
            return docs, None
        
        except Exception as e:
            print(f"  âŒ {file_path.name}: {e}")
            return [], {'file': str(file_path), 'error': str(e)}
    
    def _split_documents(self, documents: List) -> List:
        """åˆ†å‰²æ–‡æ¡£"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
        )
        return splitter.split_documents(documents)
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return ""
    
    def search(self, query: str, top_k: int = 3, 
           relevance_threshold: Optional[float] = None) -> Dict:
        """
        æœç´¢çŸ¥è¯†åº“
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æœ€å¤§ç»“æœæ•°
            relevance_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œä½äºæ­¤å€¼çš„ç»“æœä¼šè¢«è¿‡æ»¤
                                å¦‚æœä¸ºNoneï¼Œä½¿ç”¨é»˜è®¤å€¼ self.relevance_threshold
        
        Returns:
            åŒ…å«æœç´¢ç»“æœçš„å­—å…¸
        """
        if not self.vector_store:
            return {
                'question': query,
                'results': [],
                'has_results': False
            }
        
        # ä½¿ç”¨æä¾›çš„é˜ˆå€¼æˆ–é»˜è®¤å€¼
        threshold = relevance_threshold if relevance_threshold is not None else self.relevance_threshold
        
        try:
            # æœç´¢æ—¶è·å–æ›´å¤šç»“æœï¼Œç„¶åè¿‡æ»¤
            results = self.vector_store.similarity_search_with_score(query, k=top_k * 2) # åŒå€æ•°é‡ä»¥ä¾¿è¿‡æ»¤
            
            # âœ… å…³é”®ä¿®å¤ï¼šFAISS è¿”å›çš„ score æ˜¯è·ç¦»ï¼Œä¸æ˜¯ç›¸ä¼¼åº¦
            # è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼Œæ‰€ä»¥è¦ç”¨ 1 / (1 + distance) è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            filtered_results = []
            for doc, distance in results:
                # âœ… æ­£ç¡®çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šè·ç¦» â†’ ç›¸ä¼¼åº¦
                # distance èŒƒå›´ï¼š[0, âˆ)
                # similarity èŒƒå›´ï¼š(0, 1]
                # ä½¿ç”¨å…¬å¼ï¼šsimilarity = 1 / (1 + distance)
                similarity = 1 / (1 + distance)
                
                source_name = doc.metadata.get('source', 'Unknown')
                print(f"ğŸ“Š æœç´¢ç»“æœ: {source_name} (è·ç¦»: {distance:.3f}, ç›¸ä¼¼åº¦: {similarity:.3f})")
                
                # âœ… æŒ‰ç›¸å…³æ€§é˜ˆå€¼è¿‡æ»¤
                if similarity >= threshold:
                    filtered_results.append({
                        'content': doc.page_content, # æ–‡æ¡£å†…å®¹
                        'source': source_name, # æ–‡æ¡£æ¥æº
                        'score': similarity, # ä½¿ç”¨ç›¸ä¼¼åº¦ä½œä¸ºåˆ†æ•°
                        'distance': distance  # ä¿ç•™åŸå§‹è·ç¦»ç”¨äºè°ƒè¯•
                    })
                else:
                    print(f"   âŒ ç›¸ä¼¼åº¦è¿‡ä½ï¼Œè¿‡æ»¤æ‰")
            
            # âœ… åªè¿”å› top_k ä¸ªç»“æœ
            filtered_results = filtered_results[:top_k]
            
            has_results = len(filtered_results) > 0
            
            if not has_results:
                print(f"âš ï¸ æœªæ‰¾åˆ°ç›¸å…³æ€§ >= {threshold:.2%} çš„æ–‡æ¡£")
            else:
                print(f"âœ… æ‰¾åˆ° {len(filtered_results)} ä¸ªç›¸å…³æ–‡æ¡£")
            
            return {
                'question': query,
                'results': filtered_results,
                'has_results': has_results
            }
        
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {
                'question': query,
                'results': [],
                'has_results': False
            }

    def search(self, query: str, top_k: int = 3, use_reranking: bool = True) -> Dict:
        """
        æœç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒé‡æ’åºï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„ç»“æœæ•°
            use_reranking: æ˜¯å¦ä½¿ç”¨é‡æ’åºå™¨
        """
        if not self.vector_store:
            return {'question': query, 'results': [], 'has_results': False}
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šå‘é‡æ£€ç´¢ï¼ˆå¬å›æ›´å¤šå€™é€‰ï¼‰
            candidates = self.vector_store.similarity_search_with_score(
                query, 
                k=top_k * 3  # å¬å› 3 å€çš„å€™é€‰
            )
            
            # ç¬¬äºŒæ­¥ï¼šé‡æ’åº
            if use_reranking:
                try:
                    from sentence_transformers import CrossEncoder
                    
                    if not hasattr(self, 'reranker'):
                        # è·å– HuggingFace ç¼“å­˜ç›®å½•çš„ç»å¯¹è·¯å¾„
                        cache_folder = str(self.models_cache.absolute())
                        
                        try:
                            # åŠ è½½æœ¬åœ°ç¼“å­˜çš„ CrossEncoder æ¨¡å‹
                            # æ³¨æ„ï¼šHF_HUB_OFFLINE å·²åœ¨åº”ç”¨å¯åŠ¨æ—¶è®¾ç½®ä¸º '1'
                            print(f"ğŸ“¦ ä»æœ¬åœ°ç¼“å­˜åŠ è½½ CrossEncoder æ¨¡å‹...")
                            print(f"   ç¼“å­˜è·¯å¾„: {cache_folder}")
                            
                            self.reranker = CrossEncoder(
                                'cross-encoder/ms-marco-MiniLM-L-6-v2',
                                cache_folder=cache_folder
                            )
                            print(f"âœ… ä½¿ç”¨æœ¬åœ°ç¼“å­˜çš„ CrossEncoder æ¨¡å‹æˆåŠŸ!")
                            
                        except Exception as cache_error:
                            # å¦‚æœæœ¬åœ°ç¼“å­˜å¤±è´¥ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½
                            print(f"âš ï¸  æœ¬åœ°ç¼“å­˜åŠ è½½å¤±è´¥: {cache_error}")
                            print("ğŸ”„ å°è¯•ä» HuggingFace åœ¨çº¿ä¸‹è½½æ¨¡å‹...")
                            
                            # ä¸´æ—¶ç¦ç”¨ç¦»çº¿æ¨¡å¼ä»¥å…è®¸åœ¨çº¿ä¸‹è½½
                            os.environ['HF_HUB_OFFLINE'] = '0'
                            try:
                                self.reranker = CrossEncoder(
                                    'cross-encoder/ms-marco-MiniLM-L-6-v2',
                                    cache_folder=cache_folder
                                )
                                print("âœ… åœ¨çº¿ä¸‹è½½æ¨¡å‹æˆåŠŸï¼Œå·²ä¿å­˜åˆ°æœ¬åœ°ç¼“å­˜")
                                # æ¢å¤ç¦»çº¿æ¨¡å¼
                                os.environ['HF_HUB_OFFLINE'] = '1'
                            except Exception as online_error:
                                print(f"âŒ åœ¨çº¿ä¸‹è½½ä¹Ÿå¤±è´¥: {online_error}")
                                print("   ä½¿ç”¨æœ¬åœ°ç¼“å­˜æˆ–é™çº§å¤„ç†")
                                use_reranking = False  # ç¦ç”¨ Re-Ranking
                                # æ¢å¤ç¦»çº¿æ¨¡å¼
                                os.environ['HF_HUB_OFFLINE'] = '1'
                    
                    if use_reranking:  # åªæœ‰æ¨¡å‹åŠ è½½æˆåŠŸæ‰æ‰§è¡Œé‡æ’åº
                        docs = [doc for doc, _ in candidates]
                        
                        # é‡æ’åº
                        scores = self.reranker.predict([
                            (query, doc.page_content)
                            for doc in docs
                        ])
                        
                        # æŒ‰åˆ†æ•°é‡æ–°æ’åº
                        candidates = sorted(
                            zip(docs, scores),
                            key=lambda x: x[1],
                            reverse=True
                        )
                        # è½¬æ¢æ ¼å¼
                        candidates = [(doc, score) for doc, score in candidates]
                        print(f"âœ… é‡æ’åºå®Œæˆ: {len(candidates)} ä¸ªç»“æœ")
                    
                except Exception as e:
                    print(f"âš ï¸  Re-Ranking å¤±è´¥ï¼Œé™çº§åˆ°å‘é‡ç›¸ä¼¼åº¦: {e}")
                    # é™çº§å¤„ç†ï¼šç»§ç»­ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
                    pass
            
            # ç¬¬ä¸‰æ­¥ï¼šæ ¼å¼åŒ–ç»“æœï¼ˆä¸å†éœ€è¦ç¡¬é˜ˆå€¼ï¼ï¼‰
            results = []
            for doc, score in candidates[:top_k]:
                results.append({
                    'content': doc.page_content,
                    'source': doc.metadata.get('source', 'Unknown'),
                    'score': float(score),  # ç°åœ¨æ˜¯é‡æ’åºåˆ†æ•°è€Œä¸æ˜¯å‘é‡è·ç¦»
                })
            
            has_results = len(results) > 0
            
            print(f"âœ… é‡æ’åºå®Œæˆ: {len(results)} ä¸ªç»“æœ")
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result['source']} (åˆ†æ•°: {result['score']:.3f})")
            
            return {
                'question': query,
                'results': results,
                'has_results': has_results
            }
        
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            return {
                'question': query,
                'results': [],
                'has_results': False
            }

    def query(self, question: str, top_k: int = 3) -> Dict:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“
        
        Returns:
            {
                'question': str,
                'answer': str,
                'sources': list,
                'has_sources': bool  # âœ… æ–°å¢å­—æ®µï¼Œè¡¨ç¤ºæ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            }
        """
        search_results = self.search(question, top_k)
        results = search_results['results']
        has_sources = search_results['has_results']
        
        answer = "\n\n".join([
            f"ã€{doc['source']}ã€‘\n{doc['content']}"
            for doc in results
        ])
        
        sources = [doc['source'] for doc in results]
        
        # âœ… å»é‡ sources
        sources = list(dict.fromkeys(sources))
        
        return {
            'question': question,
            'answer': answer or "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹",
            'sources': sources,
            'has_sources': has_sources  # âœ… æ–°å¢ï¼šæ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
        }
    
    def get_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            self.load_vector_store()
            
            total_chunks = self.vector_store.index.ntotal if self.vector_store else 0
            files = [
                {
                    'name': filename,
                    'path': metadata.get('path', ''),
                    'added_time': metadata.get('added_time', '')
                }
                for filename, metadata in self.file_metadata.items()
            ]
            
            return {
                'total_chunks': total_chunks,
                'total_files': len(files),
                'files': files
            }
        except Exception as e:
            print(f"Error getting stats: {e}")
            return {'total_chunks': 0, 'total_files': 0, 'files': []}
    
    def clear(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            import shutil
            if self.db_path.exists():
                shutil.rmtree(self.db_path)
                self.db_path.mkdir(parents=True, exist_ok=True)
            
            self.vector_store = None
            self.file_metadata = {}
            self._save_metadata()
            print("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")
    
    def delete_document(self, filename: str):
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        if filename in self.file_metadata:
            del self.file_metadata[filename]
            self._save_metadata()
            self.load_vector_store()
    
    def add_documents_from_upload(self, files) -> Dict:
        """ä»ä¸Šä¼ çš„æ–‡ä»¶æ·»åŠ æ–‡æ¡£"""
        import tempfile
        import shutil
        from pathlib import Path
        
        temp_dir = tempfile.mkdtemp()
        file_paths = []
        processed_files = []
        
        try:
            print(f"\nğŸ“ å¼€å§‹å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå…± {len(files)} ä¸ª")
            
            for idx, file in enumerate(files):
                try:
                    filename = file.filename
                    if not filename:
                        print(f"  âš ï¸  æ–‡ä»¶ {idx+1} æ²¡æœ‰æ–‡ä»¶åï¼Œè·³è¿‡")
                        continue
                    
                    print(f"  å¤„ç†æ–‡ä»¶ {idx+1}: {filename}")
                    
                    # âœ… ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¿å­˜æ–‡ä»¶
                    temp_path = Path(temp_dir) / filename
                    file.save(str(temp_path))
                    file_paths.append(str(temp_path))
                    processed_files.append(filename)
                    print(f"    âœ… å·²ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•")
                    
                except Exception as e:
                    print(f"  âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
                    continue
            
            if not file_paths:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯ä»¥å¤„ç†")
                return {
                    'added_chunks': 0,
                    'files': [],
                    'errors': ['æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶']
                }
            
            print(f"\nğŸ“š å¼€å§‹å¤„ç†æ–‡æ¡£å‘é‡åŒ–ï¼ˆ{len(file_paths)} ä¸ªæ–‡ä»¶ï¼‰...")
            result = self.add_documents(file_paths)
            
            # ä¿å­˜æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•
            print(f"\nğŸ’¾ ä¿å­˜æ–‡ä»¶åˆ°çŸ¥è¯†åº“...")
            for file_path in file_paths:
                try:
                    path = Path(file_path)
                    dest_path = self.db_path / "documents" / path.name
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, dest_path)
                    print(f"  âœ… {path.name}")
                except Exception as e:
                    print(f"  âš ï¸  ä¿å­˜å¤±è´¥: {e}")
            
            print(f"\nâœ… ä¸Šä¼ å®Œæˆ!\n")
            return result
        
        except Exception as e:
            print(f"\nâŒ ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [str(e)]
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass
