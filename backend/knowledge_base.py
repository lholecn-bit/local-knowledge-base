# backend/knowledge_base.py

import os
import json
from typing import List, Dict, Optional, Tuple
from pathlib import Path
import hashlib
from datetime import datetime


from dotenv import load_dotenv
# åŠ è½½ç¯å¢ƒå˜é‡ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œload_dotenv() ä¼šåœ¨å½“å‰ç›®å½•æŸ¥æ‰¾ .env æ–‡ä»¶
load_dotenv()

# åœ¨æœ€å¼€å§‹è®¾ç½®ç¦»çº¿æ¨¡å¼ï¼Œä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ # TODO å¦‚ä½•ä½“ç°æœ¬åœ°ä¼˜å…ˆ

project_root = Path(__file__).parent.parent  # é¡¹ç›®æ ¹ç›®å½•
models_cache_path = project_root / 'models_cache'

os.environ['HF_HUB_OFFLINE'] = '1'
os.environ['HF_HOME'] = str(models_cache_path.absolute())
os.environ['TRANSFORMERS_CACHE'] = str((models_cache_path / 'transformers').absolute())

# V2Ray ä»£ç†åœ°å€ï¼‰
# os.environ["HTTP_PROXY"] = "http://127.0.0.1:10808"  # æµè§ˆå™¨ä»£ç†ç«¯å£
# os.environ["HTTPS_PROXY"] = "http://127.0.0.1:10808"  # æ³¨æ„ï¼šHTTPS ä»£ç†ä¹Ÿå¡« http å¼€å¤´ï¼ˆæœ¬åœ°ä»£ç†é€šç”¨ï¼‰
#

try:
    from langchain_community.document_loaders import PDFPlumberLoader, TextLoader
    from langchain.text_splitter import RecursiveCharacterTextSplitter
    from langchain_openai import OpenAIEmbeddings
    from langchain_community.vectorstores import FAISS
    LANGCHAIN_AVAILABLE = True
except ImportError as e:
    print(f"Warning: langchain components not fully installed: {e}")
    LANGCHAIN_AVAILABLE = False


class LocalKnowledgeBase:
    """æœ¬åœ°çŸ¥è¯†åº“ç®¡ç†ç±»"""
    
    def __init__(self, 
                 db_path: str = "./knowledge_db",
                 chunk_size: int = 1000,
                 chunk_overlap: int = 200,
                 openai_api_key: Optional[str] = None):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        Args:
            db_path: çŸ¥è¯†åº“æ•°æ®åº“è·¯å¾„
            chunk_size: æ–‡æœ¬å—å¤§å°
            chunk_overlap: æ–‡æœ¬å—é‡å 
            openai_api_key: OpenAI API Key (å¦‚æœä¸ºNoneï¼Œåˆ™ä»ç¯å¢ƒå˜é‡è¯»å–)
        """

        self.db_path = Path(db_path)
        self.db_path.mkdir(parents=True, exist_ok=True)
        
        # 1. åˆ›å»ºæ¨¡å‹ç¼“å­˜ç›®å½• - ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„ models_cache
        # æ³¨æ„ï¼šè¿™ä¸ä¸Šé¢è®¾ç½®çš„ HF_HOME ç¯å¢ƒå˜é‡å¿…é¡»ä¸€è‡´
        project_root = Path(__file__).parent.parent
        self.models_cache = project_root / 'models_cache'
        self.models_cache.mkdir(parents=True, exist_ok=True)
        
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.metadata_file = self.db_path / "metadata.json"

        self.reranker = None
        self.reranker_model = 'light'
        
        # 2.æ”¹ä¸ºå»¶è¿ŸåŠ è½½ï¼šä¸åœ¨ __init__ ä¸­åŠ è½½æ¨¡å‹
        # è€Œæ˜¯åœ¨ search() æ–¹æ³•ä¸­ç¬¬ä¸€æ¬¡éœ€è¦æ—¶åŠ è½½
        # è¿™æ ·å¯ä»¥ç¡®ä¿ç¯å¢ƒå˜é‡å·²ç»æ­£ç¡®è®¾ç½®


                
        # 3. æ·»åŠ ç›¸å…³æ€§é˜ˆå€¼é…ç½®
        self.relevance_threshold = 0.3  # ç›¸å…³æ€§é˜ˆå€¼ï¼ˆå¯è°ƒæ•´ï¼‰

        # 4. OpenAI API Key
        self.api_key = os.getenv("OPENAI_API_KEY")
        if openai_api_key:
            self.api_key = openai_api_key

        if not self.api_key:
            raise ValueError(
                "âŒ OPENAI_API_KEY æœªè®¾ç½®ï¼\n"
                "   è¯·åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=sk-..."
            )
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self.embeddings = None
        if LANGCHAIN_AVAILABLE:
            self.embeddings = self._init_embeddings()
            if not self.embeddings:
                print(f"âš ï¸ è­¦å‘Šï¼šEmbeddings åˆå§‹åŒ–å¤±è´¥ï¼ŒçŸ¥è¯†åº“åŠŸèƒ½å°†å—é™")
        else:
            print(f"âš ï¸ è­¦å‘Šï¼šLangChain ä¸å¯ç”¨ï¼ŒçŸ¥è¯†åº“åŠŸèƒ½å°†å—é™")
        
        # 6. åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self.vector_store = None
        if self.embeddings:
            self.load_vector_store()
        
        # 7. åŠ è½½æ–‡ä»¶å…ƒæ•°æ®
        self.file_metadata = self._load_metadata()
    
    def _init_embeddings(self):
        """åˆå§‹åŒ– OpenAI Embeddings"""
        try:
            print(f"ğŸ“¦ åˆå§‹åŒ– OpenAI Embeddings (æ¨¡å‹: text-embedding-3-small)...")
            
            # ğŸ”´ æ¸…é™¤ä»£ç†ç¯å¢ƒå˜é‡ï¼Œå› ä¸º OpenAI ä¸æ”¯æŒ SOCKS ä»£ç†
            os.environ.pop('http_proxy', None)
            os.environ.pop('https_proxy', None)
            os.environ.pop('HTTP_PROXY', None)
            os.environ.pop('HTTPS_PROXY', None)
            os.environ.pop('all_proxy', None)
            os.environ.pop('ALL_PROXY', None)
            
            # è¯»å–è‡ªå®šä¹‰ API ç«¯ç‚¹
            api_base = os.getenv('OPENAI_BASE_URL')

            embeddings = OpenAIEmbeddings(
                api_key=self.api_key,
                model="text-embedding-3-small",
                base_url=api_base
            )
            print(f"âœ… OpenAI Embeddings åˆå§‹åŒ–æˆåŠŸï¼")
            if api_base:
                print(f"   API ç«¯ç‚¹: {api_base}")
            return embeddings
        except Exception as e:
            print(f"âŒ OpenAI Embeddings åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _load_metadata(self) -> Dict:
        """åŠ è½½æ–‡ä»¶å…ƒæ•°æ®"""
        if self.metadata_file.exists():
            try:
                with open(self.metadata_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½å…ƒæ•°æ®: {e}")
        return {}
    
    def _save_metadata(self):
        """ä¿å­˜æ–‡ä»¶å…ƒæ•°æ®"""
        try:
            with open(self.metadata_file, 'w', encoding='utf-8') as f:
                json.dump(self.file_metadata, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•ä¿å­˜å…ƒæ•°æ®: {e}")
    
    def load_vector_store(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        faiss_path = self.db_path / "faiss_index"
        
        if faiss_path.exists() and self.embeddings:
            try:
                self.vector_store = FAISS.load_local(
                    str(faiss_path),
                    self.embeddings,
                    allow_dangerous_deserialization=True
                )
                print(f"âœ… å‘é‡åº“å·²åŠ è½½: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            except Exception as e:
                print(f"âš ï¸ å‘é‡åº“åŠ è½½å¤±è´¥: {e}")
                self.vector_store = None
    
    def save_vector_store(self):
        """ä¿å­˜å‘é‡æ•°æ®åº“"""
        if not self.vector_store:
            print(f"âš ï¸ å‘é‡åº“ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
            return False
        
        faiss_path = self.db_path / "faiss_index"
        try:
            self.vector_store.save_local(str(faiss_path))
            print(f"âœ… å‘é‡åº“å·²ä¿å­˜: {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            return True
        except Exception as e:
            print(f"âŒ å‘é‡åº“ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def add_documents(self, file_paths: List[str]) -> Dict:
        print(f"\nğŸ“‚ å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        """æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“"""
        if not self.embeddings:
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [{'error': 'Embeddings æœªåˆå§‹åŒ–ï¼Œæ— æ³•æ·»åŠ æ–‡æ¡£'}]
            }
        
        all_documents = []
        processed_files = {}
        errors = []
        
        print(f"\nğŸ“‚ å¼€å§‹å¤„ç† {len(file_paths)} ä¸ªæ–‡ä»¶...")
        
        for file_path in file_paths:
            path = Path(file_path)
            
            if path.is_file():
                docs, error = self._load_file(path)
                if error:
                    errors.append(error)
                else:
                    all_documents.extend(docs)
                    processed_files[str(path)] = len(docs)
            elif path.is_dir():
                for ext in ['*.pdf', '*.txt', '*.md']:
                    for file_path in path.glob(f"**/{ext}"):
                        docs, error = self._load_file(file_path)
                        if error:
                            errors.append(error)
                        else:
                            all_documents.extend(docs)
                            processed_files[str(file_path)] = len(docs)
        
        if not all_documents:
            return {
                'added_chunks': 0,
                'files': [],
                'errors': errors
            }
        
        # åˆ†å‰²æ–‡æœ¬
        print(f"âœ‚ï¸ åˆ†å‰² {len(all_documents)} ä¸ªæ–‡æ¡£...")
        chunks = self._split_documents(all_documents)
        added_chunks = len(chunks)
        print(f"âœ… åˆ†å‰²å®Œæˆ: {added_chunks} ä¸ªå—")
        
        # æ·»åŠ åˆ°å‘é‡æ•°æ®åº“
        try:
            if self.vector_store is None:
                print(f"ğŸ†• åˆ›å»ºæ–°å‘é‡åº“...")
                self.vector_store = FAISS.from_documents(chunks, self.embeddings)
            else:
                print(f"â• å‘ç°æœ‰å‘é‡åº“æ·»åŠ æ–‡æ¡£...")
                self.vector_store.add_documents(chunks)
            
            print(f"âœ… å‘é‡åº“æ›´æ–°æˆåŠŸ: ç°åœ¨å…± {self.vector_store.index.ntotal} ä¸ªå‘é‡")
            
            # ä¿å­˜å‘é‡åº“
            self.save_vector_store()
        except Exception as e:
            print(f"âŒ æ·»åŠ åˆ°å‘é‡åº“å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [{'error': f'å‘é‡åº“æ“ä½œå¤±è´¥: {e}'}]
            }
        
        # æ›´æ–°å…ƒæ•°æ®
        for file_path, doc_count in processed_files.items():
            file_name = Path(file_path).name
            self.file_metadata[file_name] = {
                'path': file_path,
                'hash': self._calculate_file_hash(file_path),
                'added_time': datetime.now().isoformat(),
                'doc_count': doc_count,
                'chunks': added_chunks
            }
        
        self._save_metadata()
        print(f"ğŸ’¾ å…ƒæ•°æ®å·²ä¿å­˜\n")
        
        return {
            'added_chunks': added_chunks,
            'files': list(processed_files.keys()),
            'errors': errors
        }
    
    def _load_file(self, file_path: Path) -> tuple:
        """åŠ è½½å•ä¸ªæ–‡ä»¶"""
        try:
            if file_path.suffix.lower() == '.pdf':
                loader = PDFPlumberLoader(str(file_path))
                docs = loader.load()
            elif file_path.suffix.lower() in ['.txt', '.md']:
                loader = TextLoader(str(file_path), encoding='utf-8')
                docs = loader.load()
            else:
                return [], {'file': str(file_path), 'error': f'ä¸æ”¯æŒçš„æ ¼å¼: {file_path.suffix}'}
            
            for doc in docs:
                doc.metadata['source'] = file_path.name
            
            print(f"  âœ… {file_path.name}: {len(docs)} ä¸ªæ–‡æ¡£")
            return docs, None
        
        except Exception as e:
            print(f"  âŒ {file_path.name}: {e}")
            return [], {'file': str(file_path), 'error': str(e)}
    
    def _split_documents(self, documents: List) -> List:
        """åˆ†å‰²æ–‡æ¡£"""
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            separators=["\n\n", "\n", "ã€‚", "ï¼Œ", " ", ""]
        )
        return splitter.split_documents(documents)
    
    def _calculate_file_hash(self, file_path: str) -> str:
        """è®¡ç®—æ–‡ä»¶å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except:
            return ""
    
    def search(self, query: str, top_k: int = 3, use_reranking: bool = True) -> Dict:
        """
        æœç´¢çŸ¥è¯†åº“ï¼ˆæ”¯æŒé‡æ’åºï¼‰
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„ç»“æœæ•°
            use_reranking: æ˜¯å¦ä½¿ç”¨é‡æ’åºå™¨
        """
        if not self.vector_store:
            print(f"çŸ¥è¯†åº“ä¸å­˜åœ¨æˆ–æœªåŠ è½½")
            return {'question': query, 'results': [], 'has_results': False}
        else:
            print(f"ğŸ” å¼€å§‹æœç´¢: '{query}' (Top {top_k}, é‡æ’åº: {'å¯ç”¨' if use_reranking else 'ç¦ç”¨'})")
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šå‘é‡æ£€ç´¢ï¼ˆå¬å›æ›´å¤šå€™é€‰ï¼‰
            candidates = self.vector_store.similarity_search_with_score(
                query, 
                k=top_k * 3  # å¬å› 3 å€çš„å€™é€‰
            )

            # ä½¿ç”¨æä¾›çš„é˜ˆå€¼æˆ–é»˜è®¤å€¼
            threshold = self.relevance_threshold

            # è·ç¦»è¶Šå°è¶Šç›¸ä¼¼ï¼Œæ‰€ä»¥è¦ç”¨ 1 / (1 + distance) è½¬æ¢ä¸ºç›¸ä¼¼åº¦
            filtered_candidates = []
            for doc, distance in candidates:
                # âœ… æ­£ç¡®çš„ç›¸ä¼¼åº¦è®¡ç®—ï¼šè·ç¦» â†’ ç›¸ä¼¼åº¦
                # distance èŒƒå›´ï¼š[0, âˆ)
                # similarity èŒƒå›´ï¼š(0, 1]
                # ä½¿ç”¨å…¬å¼ï¼šsimilarity = 1 / (1 + distance)
                similarity = 1 / (1 + distance)
                
                source_name = doc.metadata.get('source', 'Unknown')
                print(f"ğŸ“Š æœç´¢ç»“æœ: {source_name} (è·ç¦»: {distance:.3f}, ç›¸ä¼¼åº¦: {similarity:.3f})")
                
                # âœ… æŒ‰ç›¸å…³æ€§é˜ˆå€¼è¿‡æ»¤
                if similarity >= threshold:
                    filtered_candidates.append({
                        'content': doc.page_content, # æ–‡æ¡£å†…å®¹
                        'source': source_name, # æ–‡æ¡£æ¥æº
                        'score': similarity, # ä½¿ç”¨ç›¸ä¼¼åº¦ä½œä¸ºåˆ†æ•°
                        'distance': distance  # ä¿ç•™åŸå§‹è·ç¦»ç”¨äºè°ƒè¯•
                    })
                else:
                    print(f"   âŒ ç›¸ä¼¼åº¦è¿‡ä½ï¼Œè¿‡æ»¤æ‰")

            # âœ… åªè¿”å› top_k ä¸ªç»“æœ
            filtered_candidates = filtered_candidates[:top_k]
            
            # ç¬¬äºŒæ­¥ï¼šé‡æ’åº
            if use_reranking:
                try:
                    # âœ… å»¶è¿Ÿå¯¼å…¥ï¼šåœ¨ä½¿ç”¨æ—¶æ‰å¯¼å…¥
                    from sentence_transformers import CrossEncoder
                    
                    # âœ… ç¬¬ä¸€æ¬¡éœ€è¦æ—¶æ‰åŠ è½½æ¨¡å‹
                    if self.reranker is None:
                        # å®šä¹‰æ¨¡å‹æ˜ å°„
                        model_map = {
                            'light': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
                            'medium': 'BAAI/bge-reranker-base',
                            'large': 'BAAI/bge-reranker-large'
                        }
                        
                        model_name = model_map.get(self.reranker_model, model_map['light'])
                        
                        try:
                            print(f"ğŸ“¦ [å»¶è¿ŸåŠ è½½] åŠ è½½é‡æ’åºæ¨¡å‹: {model_name}...")
                            # âœ… æ˜ç¡®æŒ‡å®šç¼“å­˜ç›®å½•
                            cache_folder = str(self.models_cache.absolute())
                            
                            self.reranker = CrossEncoder(
                                model_name,
                                cache_folder=cache_folder  # âœ… æŒ‡å®šç¼“å­˜ä½ç½®
                            )
                            print(f"âœ… é‡æ’åºå™¨åŠ è½½æˆåŠŸ (ç¼“å­˜: {cache_folder})")
                        except Exception as load_error:
                            print(f"âš ï¸ é‡æ’åºå™¨åŠ è½½å¤±è´¥: {load_error}")
                            print(f"   ä½¿ç”¨åŸå§‹å‘é‡æœç´¢")
                            self.reranker = None
                    
                    # âœ… åªæœ‰æ¨¡å‹åŠ è½½æˆåŠŸæ‰æ‰§è¡Œé‡æ’åº
                    if self.reranker is not None:
                        # æå–æ–‡æ¡£å†…å®¹ï¼ˆä»å­—å…¸ä¸­è·å–ï¼‰
                        doc_contents = [cand['content'] for cand in filtered_candidates]
                        
                        # é‡æ’åº
                        scores = self.reranker.predict([
                            (query, content) for content in doc_contents
                        ])
                        
                        # ç»„åˆå€™é€‰æ–‡æ¡£å’Œåˆ†æ•°ï¼Œå¹¶æ’åº
                        ranked_pairs = list(zip(filtered_candidates, scores))
                        ranked_pairs.sort(key=lambda x: x[1], reverse=True)
                        
                        # æ›´æ–°ä¸ºæ’åºåçš„ç»“æœ
                        candidates = ranked_pairs
                        print(f"âœ… é‡æ’åºå®Œæˆ: {len(candidates)} ä¸ªç»“æœ")
                    
                except Exception as e:
                    print(f"âš ï¸  Re-Ranking å¤±è´¥ï¼Œé™çº§åˆ°å‘é‡ç›¸ä¼¼åº¦: {e}")
                    # é™çº§å¤„ç†ï¼šç»§ç»­ä½¿ç”¨å‘é‡ç›¸ä¼¼åº¦åˆ†æ•°
                    pass
            
            # ç¬¬ä¸‰æ­¥ï¼šæ ¼å¼åŒ–ç»“æœï¼ˆä¸å†éœ€è¦ç¡¬é˜ˆå€¼ï¼ï¼‰
            results = []
            for doc, score in candidates[:top_k]:
                results.append({
                    'content': doc.get('content') if isinstance(doc, dict) else doc.page_content,
                    'source': doc.get('source') if isinstance(doc, dict) else doc.metadata.get('source', 'Unknown'),
                    'score': float(score),  # ç°åœ¨æ˜¯é‡æ’åºåˆ†æ•°è€Œä¸æ˜¯å‘é‡è·ç¦»
                })
            
            has_results = len(results) > 0
            
            print(f"âœ… æœç´¢å®Œæˆ: {len(results)} ä¸ªç»“æœ")
            for i, result in enumerate(results, 1):
                print(f"   {i}. {result['source']} (åˆ†æ•°: {result['score']:.3f})")
            
            return {
                'question': query,
                'results': results,
                'has_results': has_results
            }
        
        except Exception as e:
            print(f"âŒ æœç´¢é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
            return {
                'question': query,
                'results': [],
                'has_results': False
            }

    def query(self, question: str, top_k: int = 3) -> Dict:
        """
        æŸ¥è¯¢çŸ¥è¯†åº“
        
        Returns:
            {
                'question': str,
                'answer': str,
                'sources': list,
                'has_sources': bool  # âœ… æ–°å¢å­—æ®µï¼Œè¡¨ç¤ºæ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
            }
        """
        search_results = self.search(question, top_k)
        results = search_results['results']
        has_sources = search_results['has_results']
        
        answer = "\n\n".join([
            f"ã€{doc['source']}ã€‘\n{doc['content']}"
            for doc in results
        ])
        
        sources = [doc['source'] for doc in results]
        
        # âœ… å»é‡ sources
        sources = list(dict.fromkeys(sources))
        
        return {
            'question': question,
            'answer': answer or "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³å†…å®¹",
            'sources': sources,
            'has_sources': has_sources  # âœ… æ–°å¢ï¼šæ˜¯å¦æœ‰ç›¸å…³æ–‡æ¡£
        }
    
    def get_stats(self) -> Dict:
        """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            self.load_vector_store()
            
            total_chunks = self.vector_store.index.ntotal if self.vector_store else 0
            files = [
                {
                    'name': filename,
                    'path': metadata.get('path', ''),
                    'added_time': metadata.get('added_time', '')
                }
                for filename, metadata in self.file_metadata.items()
            ]
            
            return {
                'total_chunks': total_chunks,
                'total_files': len(files),
                'files': files
            }
        except Exception as e:
            print(f"Error getting stats: {e}")
            return {'total_chunks': 0, 'total_files': 0, 'files': []}
    
    def clear(self):
        """æ¸…ç©ºçŸ¥è¯†åº“"""
        try:
            import shutil
            if self.db_path.exists():
                shutil.rmtree(self.db_path)
                self.db_path.mkdir(parents=True, exist_ok=True)
            
            self.vector_store = None
            self.file_metadata = {}
            self._save_metadata()
            print("âœ… çŸ¥è¯†åº“å·²æ¸…ç©º")
        except Exception as e:
            print(f"âŒ æ¸…ç©ºå¤±è´¥: {e}")
    
    def delete_document(self, filename: str):
        """åˆ é™¤æŒ‡å®šæ–‡æ¡£"""
        if filename in self.file_metadata:
            del self.file_metadata[filename]
            self._save_metadata()
            self.load_vector_store()
    
    def add_documents_from_upload(self, files) -> Dict:
        """ä»ä¸Šä¼ çš„æ–‡ä»¶æ·»åŠ æ–‡æ¡£"""
        import tempfile
        import shutil
        from pathlib import Path
        
        temp_dir = tempfile.mkdtemp()
        file_paths = []
        processed_files = []
        
        try:
            print(f"\nğŸ“ å¼€å§‹å¤„ç†ä¸Šä¼ çš„æ–‡ä»¶ï¼Œå…± {len(files)} ä¸ª")
            
            for idx, file in enumerate(files):
                try:
                    filename = file.filename
                    if not filename:
                        print(f"  âš ï¸  æ–‡ä»¶ {idx+1} æ²¡æœ‰æ–‡ä»¶åï¼Œè·³è¿‡")
                        continue
                    
                    print(f"  å¤„ç†æ–‡ä»¶ {idx+1}: {filename}")
                    
                    # âœ… ä½¿ç”¨ä¸´æ—¶ç›®å½•ä¿å­˜æ–‡ä»¶
                    temp_path = Path(temp_dir) / filename
                    file.save(str(temp_path))
                    file_paths.append(str(temp_path))
                    processed_files.append(filename)
                    print(f"    âœ… å·²ä¿å­˜åˆ°ä¸´æ—¶ç›®å½•")
                    
                except Exception as e:
                    print(f"  âŒ å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
                    continue
            
            if not file_paths:
                print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶å¯ä»¥å¤„ç†")
                return {
                    'added_chunks': 0,
                    'files': [],
                    'errors': ['æ²¡æœ‰æœ‰æ•ˆçš„æ–‡ä»¶']
                }
            
            print(f"\nğŸ“š å¼€å§‹å¤„ç†æ–‡æ¡£å‘é‡åŒ–ï¼ˆ{len(file_paths)} ä¸ªæ–‡ä»¶ï¼‰...")
            result = self.add_documents(file_paths)
            
            # ä¿å­˜æ–‡ä»¶åˆ°çŸ¥è¯†åº“ç›®å½•
            print(f"\nğŸ’¾ ä¿å­˜æ–‡ä»¶åˆ°çŸ¥è¯†åº“...")
            for file_path in file_paths:
                try:
                    path = Path(file_path)
                    dest_path = self.db_path / "documents" / path.name
                    dest_path.parent.mkdir(parents=True, exist_ok=True)
                    shutil.copy2(file_path, dest_path)
                    print(f"  âœ… {path.name}")
                except Exception as e:
                    print(f"  âš ï¸  ä¿å­˜å¤±è´¥: {e}")
            
            print(f"\nâœ… ä¸Šä¼ å®Œæˆ!\n")
            return result
        
        except Exception as e:
            print(f"\nâŒ ä¸Šä¼ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return {
                'added_chunks': 0,
                'files': [],
                'errors': [str(e)]
            }
        
        finally:
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            try:
                shutil.rmtree(temp_dir, ignore_errors=True)
            except:
                pass
